2025-11-05 15:08:38,451 - INFO - Strating benchmarking...
2025-11-05 15:12:25,620 - INFO - Strating benchmarking...
2025-11-05 15:13:59,196 - INFO - Strating benchmarking...
2025-11-05 15:13:59,196 - INFO - Random seed beállítva: 42
2025-11-05 15:14:26,959 - INFO - Strating benchmarking...
2025-11-05 15:14:26,960 - INFO - Random seed beállítva: 42
2025-11-05 15:16:01,307 - INFO - Strating benchmarking...
2025-11-05 15:16:01,308 - INFO - Random seed beállítva: 42
2025-11-05 15:19:09,935 - INFO - Strating benchmarking...
2025-11-05 15:19:09,935 - INFO - Random seed beállítva: 42
2025-11-05 15:19:09,936 - INFO - Load and preprocess data...
2025-11-05 15:19:09,937 - ERROR - Error during data loading: module 'preprocessing' has no attribute 'load_and_preprocess_data'
2025-11-07 11:47:41,937 - INFO - Strating benchmarking...
2025-11-07 11:47:41,937 - INFO - Random seed beállítva: 42
2025-11-07 11:47:41,938 - INFO - Load and preprocess data...
2025-11-07 11:47:41,938 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-07 11:47:41,940 - ERROR - Kritikus hiba: A 'C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\data\raw\your_raw_data.csv' fájl nem található.
2025-11-07 11:47:41,940 - ERROR - Error during data loading: [Errno 2] No such file or directory: 'C:\\Users\\Gab\\BME\\szakdolgozat\\src\\xai_fraud_detection_benchmark\\data\\raw\\your_raw_data.csv'
2025-11-08 11:44:04,561 - INFO - Strating benchmarking...
2025-11-08 11:44:04,562 - INFO - Random seed beállítva: 42
2025-11-08 11:44:04,562 - INFO - Load and preprocess data...
2025-11-08 11:44:04,563 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-08 11:44:04,563 - ERROR - Kritikus hiba: A 'C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\data\raw\your_raw_data.csv' fájl nem található.
2025-11-08 11:44:04,563 - ERROR - Error during data loading: [Errno 2] No such file or directory: 'C:\\Users\\Gab\\BME\\szakdolgozat\\src\\xai_fraud_detection_benchmark\\data\\raw\\your_raw_data.csv'
2025-11-08 11:44:18,379 - INFO - Strating benchmarking...
2025-11-08 11:44:18,380 - INFO - Random seed beállítva: 42
2025-11-08 11:44:18,380 - INFO - Load and preprocess data...
2025-11-08 11:44:18,381 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-08 11:44:29,009 - INFO - Nyers adat sikeresen betöltve innen: C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-08 11:44:30,966 - INFO - Adatok szétválasztva. Train méret: (590535, 393), Test méret: (5, 393)
2025-11-08 11:44:31,378 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-08 11:44:31,379 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-08 11:44:31,379 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-08 11:45:00,268 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-08 11:45:00,278 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-08 11:49:59,300 - INFO - Feldolgozott train/test szettek elmentve a(z) 'C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-08 11:49:59,627 - INFO - Successfuly loaded data. Train size: (590535, 530), Test size: (5, 530)
2025-11-08 11:49:59,629 - INFO - Starting iteration with following models: ['logistic_regression', 'random_forest', 'gradient_boosting']
2025-11-08 11:49:59,629 - INFO - --- Model: logistic_regression ---
2025-11-08 11:49:59,629 - INFO - Initialize model parameters from config...
2025-11-08 11:49:59,629 - INFO - Training and evaluating...
2025-11-08 11:49:59,629 - INFO - Starting model training: LogisticRegression...
2025-11-08 12:09:39,090 - INFO - LogisticRegression training finished.
2025-11-08 12:09:39,090 - INFO - Prediction on test dataset
2025-11-08 12:09:39,104 - INFO - Metrikák számítása...
2025-11-08 12:09:39,105 - WARNING - Unkonwn metric: 'f1_score'
2025-11-08 12:09:39,110 - INFO - Results (logistic_regression): {'accuracy': 1.0, 'roc_auc': nan}
2025-11-08 12:09:39,111 - INFO - Model saved to: C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\results\models\logistic_regression.pkl
2025-11-08 12:09:39,113 - ERROR - Error with model: logistic_regression: module 'config' has no attribute 'RUN_SHAP'
2025-11-08 12:09:39,113 - INFO - --- Model: random_forest ---
2025-11-08 12:09:39,113 - INFO - Initialize model parameters from config...
2025-11-08 12:09:39,113 - INFO - Training and evaluating...
2025-11-08 12:09:39,114 - INFO - Starting model training: RandomForestClassifier...
2025-11-08 12:13:23,999 - INFO - RandomForestClassifier training finished.
2025-11-08 12:13:23,999 - INFO - Prediction on test dataset
2025-11-08 12:13:24,017 - INFO - Metrikák számítása...
2025-11-08 12:13:24,018 - WARNING - Unkonwn metric: 'f1_score'
2025-11-08 12:13:24,023 - INFO - Results (random_forest): {'accuracy': 1.0, 'roc_auc': nan}
2025-11-08 12:13:24,035 - INFO - Model saved to: C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\results\models\random_forest.pkl
2025-11-08 12:13:24,035 - ERROR - Error with model: random_forest: module 'config' has no attribute 'RUN_SHAP'
2025-11-08 12:13:24,035 - INFO - --- Model: gradient_boosting ---
2025-11-08 12:13:24,036 - INFO - Initialize model parameters from config...
2025-11-08 12:13:24,036 - INFO - Training and evaluating...
2025-11-08 12:13:24,036 - INFO - Starting model training: GradientBoostingClassifier...
2025-11-08 14:47:13,210 - INFO - Strating benchmarking...
2025-11-08 14:47:13,211 - INFO - Random seed beállítva: 42
2025-11-08 14:47:13,211 - INFO - Load and preprocess data...
2025-11-08 14:47:13,211 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-08 14:47:24,289 - INFO - Nyers adat sikeresen betöltve innen: C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-08 14:47:26,373 - INFO - Adatok szétválasztva. Train méret: (472432, 393), Test méret: (118108, 393)
2025-11-08 14:47:26,802 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-08 14:47:26,803 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-08 14:47:26,803 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-08 14:47:48,508 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-08 14:47:49,837 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-08 14:52:38,643 - INFO - Feldolgozott train/test szettek elmentve a(z) 'C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-08 14:52:39,112 - INFO - Successfuly loaded data. Train size: (472432, 530), Test size: (118108, 530)
2025-11-08 14:52:39,112 - INFO - Starting iteration with following models: ['logistic_regression', 'random_forest', 'gradient_boosting']
2025-11-08 14:52:39,113 - INFO - --- Model: logistic_regression ---
2025-11-08 14:52:39,113 - INFO - Initialize model parameters from config...
2025-11-08 14:52:39,113 - INFO - Training and evaluating...
2025-11-08 14:52:39,114 - INFO - Starting model training: LogisticRegression...
2025-11-08 15:01:32,154 - INFO - LogisticRegression training finished.
2025-11-08 15:01:32,155 - INFO - Prediction on test dataset
2025-11-08 15:01:32,296 - INFO - Metrikák számítása...
2025-11-08 15:01:32,325 - WARNING - Can not calculate PR-AUC pred_proba is missing
2025-11-08 15:01:32,326 - INFO - Results (logistic_regression): {'accuracy': 0.9720086700308193, 'f1_score': 0.9648632181206812, 'pr_auc': None}
2025-11-08 15:01:32,327 - INFO - Model saved to: C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\results\models\logistic_regression.pkl
2025-11-08 15:01:32,328 - INFO - SHAP magyarázat generálása (logistic_regression)...
2025-11-08 15:01:32,328 - INFO - Starting SHAP explanation on model: logistic_regression...
2025-11-08 15:01:32,333 - ERROR - Hiba a SHAP magyarázat generálása során (logistic_regression): max_evals=500 is too low for the Permutation explainer, it must be at least 2 * num_features + 1 = 779!
Traceback (most recent call last):
  File "c:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\src\explain.py", line 51, in generate_shap_values
    shap_values_obj = explainer(X_data)
                      ^^^^^^^^^^^^^^^^^
  File "C:\Users\Gab\anaconda3\envs\szakdolgozat_env\Lib\site-packages\shap\explainers\_permutation.py", line 100, in __call__
    return super().__call__(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\Gab\anaconda3\envs\szakdolgozat_env\Lib\site-packages\shap\explainers\_explainer.py", line 366, in __call__
    row_result = self.explain_row(
                 ^^^^^^^^^^^^^^^^^
  File "C:\Users\Gab\anaconda3\envs\szakdolgozat_env\Lib\site-packages\shap\explainers\_permutation.py", line 192, in explain_row
    raise ValueError(
ValueError: max_evals=500 is too low for the Permutation explainer, it must be at least 2 * num_features + 1 = 779!
2025-11-08 15:01:32,338 - INFO - SHAP magyarázat elmentve.
2025-11-08 15:01:32,338 - INFO - --- Model: random_forest ---
2025-11-08 15:01:32,338 - INFO - Initialize model parameters from config...
2025-11-08 15:01:32,338 - INFO - Training and evaluating...
2025-11-08 15:01:32,339 - INFO - Starting model training: RandomForestClassifier...
2025-11-08 15:18:10,281 - INFO - Strating benchmarking...
2025-11-08 15:18:10,281 - INFO - Random seed beállítva: 42
2025-11-08 15:18:10,282 - INFO - Load and preprocess data...
2025-11-08 15:18:10,282 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-08 15:18:20,524 - INFO - Nyers adat sikeresen betöltve innen: C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-08 15:18:22,260 - INFO - Adatok szétválasztva. Train méret: (472432, 393), Test méret: (118108, 393)
2025-11-08 15:18:22,542 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-08 15:18:22,542 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-08 15:18:22,543 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-08 15:18:44,447 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-08 15:18:45,782 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-08 15:23:32,577 - INFO - Feldolgozott train/test szettek elmentve a(z) 'C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-08 15:23:32,879 - INFO - Successfuly loaded data. Train size: (472432, 530), Test size: (118108, 530)
2025-11-08 15:23:32,879 - INFO - Starting iteration with following models: ['logistic_regression', 'random_forest', 'gradient_boosting']
2025-11-08 15:23:32,879 - INFO - --- Model: logistic_regression ---
2025-11-08 15:23:32,879 - INFO - Initialize model parameters from config...
2025-11-08 15:23:32,880 - INFO - Training and evaluating...
2025-11-08 15:23:32,880 - INFO - Starting model training: LogisticRegression...
2025-11-08 15:32:23,542 - INFO - LogisticRegression training finished.
2025-11-08 15:32:23,542 - INFO - Prediction on test dataset
2025-11-08 15:32:23,679 - INFO - Metrikák számítása...
2025-11-08 15:32:23,702 - WARNING - Can not calculate PR-AUC pred_proba is missing
2025-11-08 15:32:23,702 - INFO - Results (logistic_regression): {'accuracy': 0.9720086700308193, 'f1_score': 0.9648632181206812, 'pr_auc': None}
2025-11-08 15:32:23,703 - INFO - Model saved to: C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\results\models\logistic_regression.pkl
2025-11-08 15:32:23,703 - INFO - SHAP magyarázat generálása (logistic_regression)...
2025-11-08 15:32:23,703 - ERROR - Error with model: logistic_regression: generate_shap_values() got an unexpected keyword argument 'X_data_to_explain'
2025-11-08 15:32:23,703 - INFO - --- Model: random_forest ---
2025-11-08 15:32:23,703 - INFO - Initialize model parameters from config...
2025-11-08 15:32:23,703 - INFO - Training and evaluating...
2025-11-08 15:32:23,705 - INFO - Starting model training: RandomForestClassifier...
2025-11-08 15:33:34,252 - INFO - Strating benchmarking...
2025-11-08 15:33:34,253 - INFO - Random seed beállítva: 42
2025-11-08 15:33:34,254 - INFO - Load and preprocess data...
2025-11-08 15:33:34,254 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-08 15:33:46,324 - INFO - Nyers adat sikeresen betöltve innen: C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-08 15:33:48,287 - INFO - Adatok szétválasztva. Train méret: (472432, 393), Test méret: (118108, 393)
2025-11-08 15:33:48,641 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-08 15:33:48,642 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-08 15:33:48,642 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-08 15:34:13,073 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-08 15:34:14,791 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-08 15:39:29,220 - INFO - Feldolgozott train/test szettek elmentve a(z) 'C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-08 15:39:29,497 - INFO - Successfuly loaded data. Train size: (472432, 530), Test size: (118108, 530)
2025-11-08 15:39:29,498 - INFO - Starting iteration with following models: ['logistic_regression', 'random_forest', 'gradient_boosting']
2025-11-08 15:39:29,498 - INFO - --- Model: logistic_regression ---
2025-11-08 15:39:29,498 - INFO - Initialize model parameters from config...
2025-11-08 15:39:29,498 - INFO - Training and evaluating...
2025-11-08 15:39:29,499 - INFO - Starting model training: LogisticRegression...
2025-11-08 15:48:08,478 - INFO - LogisticRegression training finished.
2025-11-08 15:48:08,478 - INFO - Prediction on test dataset
2025-11-08 15:48:08,616 - INFO - Metrikák számítása...
2025-11-08 15:48:08,643 - WARNING - Can not calculate PR-AUC pred_proba is missing
2025-11-08 15:48:08,643 - INFO - Results (logistic_regression): {'accuracy': 0.9720086700308193, 'f1_score': 0.9648632181206812, 'pr_auc': None}
2025-11-08 15:48:08,644 - INFO - Model saved to: C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\results\models\logistic_regression.pkl
2025-11-08 15:48:08,644 - INFO - SHAP magyarázat generálása (logistic_regression)...
2025-11-08 15:48:08,644 - INFO - Starting SHAP explanation on model: logistic_regression...
2025-11-08 15:48:08,660 - INFO - Calculating SHAP values...
2025-11-08 15:48:12,847 - INFO - SHAP calculation finished.
2025-11-08 15:48:12,883 - INFO - SHAP magyarázat elmentve.
2025-11-08 15:48:12,883 - INFO - --- Model: random_forest ---
2025-11-08 15:48:12,884 - INFO - Initialize model parameters from config...
2025-11-08 15:48:12,884 - INFO - Training and evaluating...
2025-11-08 15:48:12,884 - INFO - Starting model training: RandomForestClassifier...
2025-11-08 15:51:04,714 - INFO - RandomForestClassifier training finished.
2025-11-08 15:51:04,715 - INFO - Prediction on test dataset
2025-11-08 15:51:06,126 - INFO - Metrikák számítása...
2025-11-08 15:51:06,148 - WARNING - Can not calculate PR-AUC pred_proba is missing
2025-11-08 15:51:06,149 - INFO - Results (random_forest): {'accuracy': 0.9733887628272428, 'f1_score': 0.9660632551414539, 'pr_auc': None}
2025-11-08 15:51:06,168 - INFO - Model saved to: C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\results\models\random_forest.pkl
2025-11-08 15:51:06,168 - INFO - SHAP magyarázat generálása (random_forest)...
2025-11-08 15:51:06,168 - INFO - Starting SHAP explanation on model: random_forest...
2025-11-08 15:51:06,199 - INFO - Calculating SHAP values...
2025-11-08 16:06:39,583 - ERROR - Hiba a SHAP magyarázat generálása során (random_forest): Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.721581, while the model output was 0.703335. If this difference is acceptable you can set check_additivity=False to disable this check.
Traceback (most recent call last):
  File "c:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\src\explain.py", line 67, in generate_shap_values
    shap_values_obj = explainer(X_to_explain)
                      ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Gab\anaconda3\envs\szakdolgozat_env\Lib\site-packages\shap\explainers\_tree.py", line 380, in __call__
    v = self.shap_values(X, y=y, from_call=True, check_additivity=check_additivity, approximate=approximate)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Gab\anaconda3\envs\szakdolgozat_env\Lib\site-packages\shap\explainers\_tree.py", line 672, in shap_values
    self.assert_additivity(out, self.model.predict(X))
  File "C:\Users\Gab\anaconda3\envs\szakdolgozat_env\Lib\site-packages\shap\explainers\_tree.py", line 856, in assert_additivity
    check_sum(self.expected_value[i] + phi[i].sum(-1), model_output[:, i])
  File "C:\Users\Gab\anaconda3\envs\szakdolgozat_env\Lib\site-packages\shap\explainers\_tree.py", line 852, in check_sum
    raise ExplainerError(err_msg)
shap.utils._exceptions.ExplainerError: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.721581, while the model output was 0.703335. If this difference is acceptable you can set check_additivity=False to disable this check.
2025-11-08 16:06:39,652 - INFO - SHAP magyarázat elmentve.
2025-11-08 16:06:39,652 - INFO - --- Model: gradient_boosting ---
2025-11-08 16:06:39,653 - INFO - Initialize model parameters from config...
2025-11-08 16:06:39,653 - INFO - Training and evaluating...
2025-11-08 16:06:39,653 - INFO - Starting model training: GradientBoostingClassifier...
2025-11-08 16:54:29,480 - INFO - GradientBoostingClassifier training finished.
2025-11-08 16:54:29,480 - INFO - Prediction on test dataset
2025-11-08 16:54:30,356 - INFO - Metrikák számítása...
2025-11-08 16:54:30,374 - WARNING - Can not calculate PR-AUC pred_proba is missing
2025-11-08 16:54:30,378 - INFO - Results (gradient_boosting): {'accuracy': 0.9745063839875369, 'f1_score': 0.9688491915681886, 'pr_auc': None}
2025-11-08 16:54:30,386 - INFO - Model saved to: C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\results\models\gradient_boosting.pkl
2025-11-08 16:54:30,386 - INFO - SHAP magyarázat generálása (gradient_boosting)...
2025-11-08 16:54:30,387 - INFO - Starting SHAP explanation on model: gradient_boosting...
2025-11-08 16:54:30,403 - INFO - Calculating SHAP values...
2025-11-08 16:56:35,012 - INFO - SHAP calculation finished.
2025-11-08 16:56:35,041 - INFO - SHAP magyarázat elmentve.
2025-11-08 16:56:35,041 - INFO - Benchmarking finished. Saving results...
2025-11-08 16:56:35,043 - INFO - Metrics summary saved to: C:\Users\Gab\BME\szakdolgozat\src\xai_fraud_detection_benchmark\results\benchmark_summary.json
2025-11-08 16:56:35,043 - INFO - Benchmarking finished.
2025-11-10 09:46:54,417 - INFO - Strating benchmarking...
2025-11-10 09:46:54,419 - INFO - Random seed beállítva: 42
2025-11-10 09:46:54,420 - INFO - Load and preprocess data...
2025-11-10 09:46:54,420 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-10 09:46:54,929 - INFO - Nyers adat sikeresen betöltve innen: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-10 09:46:55,008 - INFO - Adatok szétválasztva. Train méret: (8000, 393), Test méret: (2000, 393)
2025-11-10 09:46:55,033 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-10 09:46:55,035 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-10 09:46:55,037 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-10 09:46:55,680 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-10 09:46:55,749 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-10 09:47:03,574 - INFO - Feldolgozott train/test szettek elmentve a(z) 'E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-10 09:47:03,587 - INFO - Successfuly loaded data. Train size: (8000, 505), Test size: (2000, 505)
2025-11-10 09:47:03,588 - INFO - Starting iteration with following models: ['logistic_regression', 'random_forest', 'gradient_boosting']
2025-11-10 09:47:03,588 - INFO - --- Model: logistic_regression ---
2025-11-10 09:47:03,589 - INFO - Initialize model parameters from config...
2025-11-10 09:47:03,589 - INFO - Training and evaluating...
2025-11-10 09:47:03,589 - INFO - Starting model training: LogisticRegression...
2025-11-10 09:47:10,638 - INFO - LogisticRegression training finished.
2025-11-10 09:47:10,639 - INFO - Prediction on test dataset
2025-11-10 09:47:10,649 - INFO - Metrikák számítása...
2025-11-10 09:47:10,658 - WARNING - Can not calculate PR-AUC pred_proba is missing
2025-11-10 09:47:10,658 - INFO - Results (logistic_regression): {'accuracy': 0.9645, 'f1_score': 0.9599949731165981, 'pr_auc': None}
2025-11-10 09:47:10,661 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\logistic_regression.pkl
2025-11-10 09:47:10,661 - INFO - SHAP magyarázat generálása (logistic_regression)...
2025-11-10 09:47:10,661 - INFO - Starting SHAP explanation on model: logistic_regression...
2025-11-10 09:47:10,665 - INFO - Calculating SHAP values...
2025-11-10 09:47:10,778 - INFO - SHAP calculation finished.
2025-11-10 09:47:10,781 - INFO - SHAP magyarázat elmentve.
2025-11-10 09:47:10,781 - INFO - --- Model: random_forest ---
2025-11-10 09:47:10,781 - INFO - Initialize model parameters from config...
2025-11-10 09:47:10,782 - INFO - Training and evaluating...
2025-11-10 09:47:10,782 - INFO - Starting model training: RandomForestClassifier...
2025-11-10 09:47:13,629 - INFO - RandomForestClassifier training finished.
2025-11-10 09:47:13,630 - INFO - Prediction on test dataset
2025-11-10 09:47:13,666 - INFO - Metrikák számítása...
2025-11-10 09:47:13,671 - WARNING - Can not calculate PR-AUC pred_proba is missing
2025-11-10 09:47:13,671 - INFO - Results (random_forest): {'accuracy': 0.9705, 'f1_score': 0.9617141505914562, 'pr_auc': None}
2025-11-10 09:47:13,680 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\random_forest.pkl
2025-11-10 09:47:13,680 - INFO - SHAP magyarázat generálása (random_forest)...
2025-11-10 09:47:13,680 - INFO - Starting SHAP explanation on model: random_forest...
2025-11-10 09:47:13,691 - INFO - Calculating SHAP values...
2025-11-10 09:47:26,981 - ERROR - Hiba a SHAP magyarázat generálása során (random_forest): Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.254216, while the model output was 0.234216. If this difference is acceptable you can set check_additivity=False to disable this check.
Traceback (most recent call last):
  File "e:\Users\G9179\source\xai_fraud_detection_benchmark\src\explain.py", line 67, in generate_shap_values
    shap_values_obj = explainer(X_to_explain)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 380, in __call__
    v = self.shap_values(X, y=y, from_call=True, check_additivity=check_additivity, approximate=approximate)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 677, in shap_values
    self.assert_additivity(out, self.model.predict(X))
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 862, in assert_additivity
    check_sum(self.expected_value[i] + phi[i].sum(-1), model_output[:, i])
    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 858, in check_sum
    raise ExplainerError(err_msg)
shap.utils._exceptions.ExplainerError: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.254216, while the model output was 0.234216. If this difference is acceptable you can set check_additivity=False to disable this check.
2025-11-10 09:47:26,988 - INFO - SHAP magyarázat elmentve.
2025-11-10 09:47:26,989 - INFO - --- Model: gradient_boosting ---
2025-11-10 09:47:26,989 - INFO - Initialize model parameters from config...
2025-11-10 09:47:26,989 - INFO - Training and evaluating...
2025-11-10 09:47:26,989 - INFO - Starting model training: GradientBoostingClassifier...
2025-11-10 09:48:05,751 - INFO - GradientBoostingClassifier training finished.
2025-11-10 09:48:05,752 - INFO - Prediction on test dataset
2025-11-10 09:48:05,766 - INFO - Metrikák számítása...
2025-11-10 09:48:05,771 - WARNING - Can not calculate PR-AUC pred_proba is missing
2025-11-10 09:48:05,773 - INFO - Results (gradient_boosting): {'accuracy': 0.9695, 'f1_score': 0.9637639252512067, 'pr_auc': None}
2025-11-10 09:48:05,780 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\gradient_boosting.pkl
2025-11-10 09:48:05,781 - INFO - SHAP magyarázat generálása (gradient_boosting)...
2025-11-10 09:48:05,781 - INFO - Starting SHAP explanation on model: gradient_boosting...
2025-11-10 09:48:05,793 - INFO - Calculating SHAP values...
2025-11-10 09:48:08,990 - ERROR - Hiba a SHAP magyarázat generálása során (gradient_boosting): Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 4.838153, while the model output was 4.939068. If this difference is acceptable you can set check_additivity=False to disable this check.
Traceback (most recent call last):
  File "e:\Users\G9179\source\xai_fraud_detection_benchmark\src\explain.py", line 67, in generate_shap_values
    shap_values_obj = explainer(X_to_explain)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 380, in __call__
    v = self.shap_values(X, y=y, from_call=True, check_additivity=check_additivity, approximate=approximate)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 677, in shap_values
    self.assert_additivity(out, self.model.predict(X))
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 864, in assert_additivity
    check_sum(self.expected_value + phi.sum(-1), model_output)
    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 858, in check_sum
    raise ExplainerError(err_msg)
shap.utils._exceptions.ExplainerError: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 4.838153, while the model output was 4.939068. If this difference is acceptable you can set check_additivity=False to disable this check.
2025-11-10 09:48:08,996 - INFO - SHAP magyarázat elmentve.
2025-11-10 09:48:08,996 - INFO - Benchmarking finished. Saving results...
2025-11-10 09:48:08,998 - INFO - Metrics summary saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\benchmark_summary.json
2025-11-10 09:48:08,998 - INFO - Benchmarking finished.
2025-11-10 09:53:38,013 - INFO - Strating benchmarking...
2025-11-10 09:53:38,014 - INFO - Random seed beállítva: 42
2025-11-10 09:53:38,015 - INFO - Load and preprocess data...
2025-11-10 09:53:38,016 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-10 09:53:38,476 - INFO - Nyers adat sikeresen betöltve innen: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-10 09:53:38,548 - INFO - Adatok szétválasztva. Train méret: (8000, 393), Test méret: (2000, 393)
2025-11-10 09:53:38,571 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-10 09:53:38,575 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-10 09:53:38,576 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-10 09:53:39,315 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-10 09:53:39,380 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-10 09:53:48,099 - INFO - Feldolgozott train/test szettek elmentve a(z) 'E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-10 09:53:48,117 - INFO - Successfuly loaded data. Train size: (8000, 505), Test size: (2000, 505)
2025-11-10 09:53:48,118 - INFO - Starting iteration with following models: ['logistic_regression', 'random_forest', 'gradient_boosting']
2025-11-10 09:53:48,118 - INFO - --- Model: logistic_regression ---
2025-11-10 09:53:48,119 - INFO - Initialize model parameters from config...
2025-11-10 09:53:48,119 - INFO - Training and evaluating...
2025-11-10 09:53:48,119 - INFO - Starting model training: LogisticRegression...
2025-11-10 09:53:55,820 - INFO - LogisticRegression training finished.
2025-11-10 09:53:55,821 - INFO - Prediction on test dataset
2025-11-10 09:53:55,827 - INFO - Metrikák számítása...
2025-11-10 09:53:55,834 - WARNING - Can not calculate PR-AUC pred_proba is missing
2025-11-10 09:53:55,835 - INFO - Results (logistic_regression): {'accuracy': 0.9645, 'f1_score': 0.9599949731165981, 'pr_auc': None}
2025-11-10 09:53:55,837 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\logistic_regression.pkl
2025-11-10 09:53:55,838 - INFO - SHAP magyarázat generálása (logistic_regression)...
2025-11-10 09:53:55,838 - INFO - Starting SHAP explanation on model: logistic_regression...
2025-11-10 09:53:55,840 - INFO - Calculating SHAP values...
2025-11-10 09:53:55,976 - INFO - SHAP calculation finished.
2025-11-10 09:53:55,977 - INFO - Generating SHAP beeswarm plot...
2025-11-10 09:53:56,435 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_beeswarm.png
2025-11-10 09:53:56,435 - INFO - Generating SHAP bar plot...
2025-11-10 09:53:56,759 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_bar.png
2025-11-10 09:53:56,763 - INFO - SHAP magyarázat elmentve.
2025-11-10 09:53:56,763 - INFO - --- Model: random_forest ---
2025-11-10 09:53:56,765 - INFO - Initialize model parameters from config...
2025-11-10 09:53:56,765 - INFO - Training and evaluating...
2025-11-10 09:53:56,765 - INFO - Starting model training: RandomForestClassifier...
2025-11-10 09:53:59,420 - INFO - RandomForestClassifier training finished.
2025-11-10 09:53:59,420 - INFO - Prediction on test dataset
2025-11-10 09:53:59,456 - INFO - Metrikák számítása...
2025-11-10 09:53:59,464 - WARNING - Can not calculate PR-AUC pred_proba is missing
2025-11-10 09:53:59,467 - INFO - Results (random_forest): {'accuracy': 0.9705, 'f1_score': 0.9617141505914562, 'pr_auc': None}
2025-11-10 09:53:59,477 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\random_forest.pkl
2025-11-10 09:53:59,478 - INFO - SHAP magyarázat generálása (random_forest)...
2025-11-10 09:53:59,484 - INFO - Starting SHAP explanation on model: random_forest...
2025-11-10 09:53:59,496 - INFO - Calculating SHAP values...
2025-11-10 09:54:13,178 - ERROR - Hiba a SHAP magyarázat generálása során (random_forest): Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.254216, while the model output was 0.234216. If this difference is acceptable you can set check_additivity=False to disable this check.
Traceback (most recent call last):
  File "e:\Users\G9179\source\xai_fraud_detection_benchmark\src\explain.py", line 67, in generate_shap_values
    shap_values_obj = explainer(X_to_explain)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 380, in __call__
    v = self.shap_values(X, y=y, from_call=True, check_additivity=check_additivity, approximate=approximate)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 677, in shap_values
    self.assert_additivity(out, self.model.predict(X))
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 862, in assert_additivity
    check_sum(self.expected_value[i] + phi[i].sum(-1), model_output[:, i])
    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 858, in check_sum
    raise ExplainerError(err_msg)
shap.utils._exceptions.ExplainerError: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.254216, while the model output was 0.234216. If this difference is acceptable you can set check_additivity=False to disable this check.
2025-11-10 09:54:13,185 - INFO - SHAP magyarázat elmentve.
2025-11-10 09:54:13,186 - INFO - --- Model: gradient_boosting ---
2025-11-10 09:54:13,186 - INFO - Initialize model parameters from config...
2025-11-10 09:54:13,186 - INFO - Training and evaluating...
2025-11-10 09:54:13,187 - INFO - Starting model training: GradientBoostingClassifier...
2025-11-10 09:54:52,410 - INFO - GradientBoostingClassifier training finished.
2025-11-10 09:54:52,411 - INFO - Prediction on test dataset
2025-11-10 09:54:52,425 - INFO - Metrikák számítása...
2025-11-10 09:54:52,430 - WARNING - Can not calculate PR-AUC pred_proba is missing
2025-11-10 09:54:52,430 - INFO - Results (gradient_boosting): {'accuracy': 0.9695, 'f1_score': 0.9637639252512067, 'pr_auc': None}
2025-11-10 09:54:52,436 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\gradient_boosting.pkl
2025-11-10 09:54:52,436 - INFO - SHAP magyarázat generálása (gradient_boosting)...
2025-11-10 09:54:52,437 - INFO - Starting SHAP explanation on model: gradient_boosting...
2025-11-10 09:54:52,447 - INFO - Calculating SHAP values...
2025-11-10 09:54:55,300 - ERROR - Hiba a SHAP magyarázat generálása során (gradient_boosting): Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 4.838153, while the model output was 4.939068. If this difference is acceptable you can set check_additivity=False to disable this check.
Traceback (most recent call last):
  File "e:\Users\G9179\source\xai_fraud_detection_benchmark\src\explain.py", line 67, in generate_shap_values
    shap_values_obj = explainer(X_to_explain)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 380, in __call__
    v = self.shap_values(X, y=y, from_call=True, check_additivity=check_additivity, approximate=approximate)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 677, in shap_values
    self.assert_additivity(out, self.model.predict(X))
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 864, in assert_additivity
    check_sum(self.expected_value + phi.sum(-1), model_output)
    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 858, in check_sum
    raise ExplainerError(err_msg)
shap.utils._exceptions.ExplainerError: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 4.838153, while the model output was 4.939068. If this difference is acceptable you can set check_additivity=False to disable this check.
2025-11-10 09:54:55,306 - INFO - SHAP magyarázat elmentve.
2025-11-10 09:54:55,307 - INFO - Benchmarking finished. Saving results...
2025-11-10 09:54:55,308 - INFO - Metrics summary saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\benchmark_summary.json
2025-11-10 09:54:55,309 - INFO - Benchmarking finished.
2025-11-10 11:24:49,612 - INFO - Strating benchmarking...
2025-11-10 11:24:49,613 - INFO - Random seed beállítva: 42
2025-11-10 11:24:49,614 - INFO - Load and preprocess data...
2025-11-10 11:24:49,614 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-10 11:24:50,092 - INFO - Nyers adat sikeresen betöltve innen: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-10 11:24:50,162 - INFO - Adatok szétválasztva. Train méret: (8000, 393), Test méret: (2000, 393)
2025-11-10 11:24:50,184 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-10 11:24:50,187 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-10 11:24:50,190 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-10 11:24:50,917 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-10 11:24:50,982 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-10 11:24:58,879 - INFO - Feldolgozott train/test szettek elmentve a(z) 'E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-10 11:24:58,899 - INFO - Successfuly loaded data. Train size: (8000, 505), Test size: (2000, 505)
2025-11-10 11:24:58,900 - INFO - Starting iteration with following models: ['logistic_regression', 'random_forest', 'gradient_boosting']
2025-11-10 11:24:58,900 - INFO - --- Model: logistic_regression ---
2025-11-10 11:24:58,901 - INFO - Initialize model parameters from config...
2025-11-10 11:24:58,901 - INFO - Training and evaluating...
2025-11-10 11:24:58,901 - INFO - Starting model training: LogisticRegression...
2025-11-10 11:25:05,862 - INFO - LogisticRegression training finished.
2025-11-10 11:25:05,862 - INFO - Prediction on test dataset
2025-11-10 11:25:05,869 - INFO - Metrikák számítása...
2025-11-10 11:25:05,874 - WARNING - Can not calculate PR-AUC pred_proba is missing
2025-11-10 11:25:05,874 - INFO - Results (logistic_regression): {'accuracy': 0.9645, 'f1_score': 0.9599949731165981, 'pr_auc': None}
2025-11-10 11:25:05,876 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\logistic_regression.pkl
2025-11-10 11:25:05,876 - INFO - SHAP magyarázat generálása (logistic_regression)...
2025-11-10 11:25:05,876 - INFO - Starting SHAP explanation on model: logistic_regression...
2025-11-10 11:25:05,879 - INFO - Calculating SHAP values...
2025-11-10 11:25:05,968 - INFO - SHAP calculation finished.
2025-11-10 11:25:05,969 - INFO - Generating SHAP beeswarm plot...
2025-11-10 11:25:06,269 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_beeswarm.png
2025-11-10 11:25:06,269 - INFO - Generating SHAP bar plot...
2025-11-10 11:25:06,532 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_bar.png
2025-11-10 11:25:06,535 - INFO - SHAP magyarázat elmentve.
2025-11-10 11:25:06,535 - INFO - --- Model: random_forest ---
2025-11-10 11:25:06,535 - INFO - Initialize model parameters from config...
2025-11-10 11:25:06,535 - INFO - Training and evaluating...
2025-11-10 11:25:06,536 - INFO - Starting model training: RandomForestClassifier...
2025-11-10 11:25:09,139 - INFO - RandomForestClassifier training finished.
2025-11-10 11:25:09,140 - INFO - Prediction on test dataset
2025-11-10 11:25:09,173 - INFO - Metrikák számítása...
2025-11-10 11:25:09,179 - WARNING - Can not calculate PR-AUC pred_proba is missing
2025-11-10 11:25:09,179 - INFO - Results (random_forest): {'accuracy': 0.9705, 'f1_score': 0.9617141505914562, 'pr_auc': None}
2025-11-10 11:25:09,186 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\random_forest.pkl
2025-11-10 11:25:09,187 - INFO - SHAP magyarázat generálása (random_forest)...
2025-11-10 11:25:09,187 - INFO - Starting SHAP explanation on model: random_forest...
2025-11-10 11:25:09,198 - INFO - Calculating SHAP values...
2025-11-10 11:25:22,781 - ERROR - Hiba a SHAP magyarázat generálása során (random_forest): Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.254216, while the model output was 0.234216. If this difference is acceptable you can set check_additivity=False to disable this check.
Traceback (most recent call last):
  File "e:\Users\G9179\source\xai_fraud_detection_benchmark\src\explain.py", line 67, in generate_shap_values
    shap_values_obj = explainer(X_to_explain)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 380, in __call__
    v = self.shap_values(X, y=y, from_call=True, check_additivity=check_additivity, approximate=approximate)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 677, in shap_values
    self.assert_additivity(out, self.model.predict(X))
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 862, in assert_additivity
    check_sum(self.expected_value[i] + phi[i].sum(-1), model_output[:, i])
    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 858, in check_sum
    raise ExplainerError(err_msg)
shap.utils._exceptions.ExplainerError: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.254216, while the model output was 0.234216. If this difference is acceptable you can set check_additivity=False to disable this check.
2025-11-10 11:25:22,790 - INFO - SHAP magyarázat elmentve.
2025-11-10 11:25:22,791 - INFO - --- Model: gradient_boosting ---
2025-11-10 11:25:22,791 - INFO - Initialize model parameters from config...
2025-11-10 11:25:22,791 - INFO - Training and evaluating...
2025-11-10 11:25:22,791 - INFO - Starting model training: GradientBoostingClassifier...
2025-11-10 11:26:02,134 - INFO - GradientBoostingClassifier training finished.
2025-11-10 11:26:02,135 - INFO - Prediction on test dataset
2025-11-10 11:26:02,156 - INFO - Metrikák számítása...
2025-11-10 11:26:02,161 - WARNING - Can not calculate PR-AUC pred_proba is missing
2025-11-10 11:26:02,163 - INFO - Results (gradient_boosting): {'accuracy': 0.9695, 'f1_score': 0.9637639252512067, 'pr_auc': None}
2025-11-10 11:26:02,172 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\gradient_boosting.pkl
2025-11-10 11:26:02,172 - INFO - SHAP magyarázat generálása (gradient_boosting)...
2025-11-10 11:26:02,173 - INFO - Starting SHAP explanation on model: gradient_boosting...
2025-11-10 11:26:02,185 - INFO - Calculating SHAP values...
2025-11-10 11:26:05,194 - ERROR - Hiba a SHAP magyarázat generálása során (gradient_boosting): Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 4.838153, while the model output was 4.939068. If this difference is acceptable you can set check_additivity=False to disable this check.
Traceback (most recent call last):
  File "e:\Users\G9179\source\xai_fraud_detection_benchmark\src\explain.py", line 67, in generate_shap_values
    shap_values_obj = explainer(X_to_explain)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 380, in __call__
    v = self.shap_values(X, y=y, from_call=True, check_additivity=check_additivity, approximate=approximate)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 677, in shap_values
    self.assert_additivity(out, self.model.predict(X))
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 864, in assert_additivity
    check_sum(self.expected_value + phi.sum(-1), model_output)
    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 858, in check_sum
    raise ExplainerError(err_msg)
shap.utils._exceptions.ExplainerError: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 4.838153, while the model output was 4.939068. If this difference is acceptable you can set check_additivity=False to disable this check.
2025-11-10 11:26:05,200 - INFO - SHAP magyarázat elmentve.
2025-11-10 11:26:05,200 - INFO - Benchmarking finished. Saving results...
2025-11-10 11:26:05,202 - INFO - Metrics summary saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\benchmark_summary.json
2025-11-10 11:26:05,202 - INFO - Benchmarking finished.
2025-11-10 11:29:36,159 - INFO - Strating benchmarking...
2025-11-10 11:29:36,159 - INFO - Random seed beállítva: 42
2025-11-10 11:29:36,160 - INFO - Load and preprocess data...
2025-11-10 11:29:36,161 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-10 11:29:36,627 - INFO - Nyers adat sikeresen betöltve innen: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-10 11:29:36,692 - INFO - Adatok szétválasztva. Train méret: (8000, 393), Test méret: (2000, 393)
2025-11-10 11:29:36,714 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-10 11:29:36,716 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-10 11:29:36,717 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-10 11:29:37,330 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-10 11:29:37,393 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-10 11:29:45,220 - INFO - Feldolgozott train/test szettek elmentve a(z) 'E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-10 11:29:45,234 - INFO - Successfuly loaded data. Train size: (8000, 505), Test size: (2000, 505)
2025-11-10 11:29:45,234 - INFO - Starting iteration with following models: ['logistic_regression', 'random_forest', 'gradient_boosting']
2025-11-10 11:29:45,235 - INFO - --- Model: logistic_regression ---
2025-11-10 11:29:45,235 - INFO - Initialize model parameters from config...
2025-11-10 11:29:45,235 - INFO - Training and evaluating...
2025-11-10 11:29:45,235 - INFO - Starting model training: LogisticRegression...
2025-11-10 11:29:52,502 - INFO - LogisticRegression training finished.
2025-11-10 11:29:52,503 - INFO - Prediction on test dataset
2025-11-10 11:29:52,520 - INFO - Metrikák számítása...
2025-11-10 11:29:52,531 - INFO - Results (logistic_regression): {'accuracy': 0.9645, 'f1_score': 0.9599949731165981, 'pr_auc': 0.27440597908410147}
2025-11-10 11:29:52,533 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\logistic_regression.pkl
2025-11-10 11:29:52,533 - INFO - SHAP magyarázat generálása (logistic_regression)...
2025-11-10 11:29:52,533 - INFO - Starting SHAP explanation on model: logistic_regression...
2025-11-10 11:29:52,535 - INFO - Calculating SHAP values...
2025-11-10 11:29:52,631 - INFO - SHAP calculation finished.
2025-11-10 11:29:52,631 - INFO - Generating SHAP beeswarm plot...
2025-11-10 11:29:52,930 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_beeswarm.png
2025-11-10 11:29:52,931 - INFO - Generating SHAP bar plot...
2025-11-10 11:29:53,192 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_bar.png
2025-11-10 11:29:53,196 - INFO - SHAP magyarázat elmentve.
2025-11-10 11:29:53,196 - INFO - --- Model: random_forest ---
2025-11-10 11:29:53,197 - INFO - Initialize model parameters from config...
2025-11-10 11:29:53,197 - INFO - Training and evaluating...
2025-11-10 11:29:53,197 - INFO - Starting model training: RandomForestClassifier...
2025-11-10 11:29:56,053 - INFO - RandomForestClassifier training finished.
2025-11-10 11:29:56,054 - INFO - Prediction on test dataset
2025-11-10 11:29:56,147 - INFO - Metrikák számítása...
2025-11-10 11:29:56,157 - INFO - Results (random_forest): {'accuracy': 0.9705, 'f1_score': 0.9617141505914562, 'pr_auc': 0.3807479382619139}
2025-11-10 11:29:56,164 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\random_forest.pkl
2025-11-10 11:29:56,165 - INFO - SHAP magyarázat generálása (random_forest)...
2025-11-10 11:29:56,165 - INFO - Starting SHAP explanation on model: random_forest...
2025-11-10 11:29:56,182 - INFO - Calculating SHAP values...
2025-11-10 11:30:12,789 - ERROR - Hiba a SHAP magyarázat generálása során (random_forest): Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.254216, while the model output was 0.234216. If this difference is acceptable you can set check_additivity=False to disable this check.
Traceback (most recent call last):
  File "e:\Users\G9179\source\xai_fraud_detection_benchmark\src\explain.py", line 67, in generate_shap_values
    shap_values_obj = explainer(X_to_explain)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 380, in __call__
    v = self.shap_values(X, y=y, from_call=True, check_additivity=check_additivity, approximate=approximate)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 677, in shap_values
    self.assert_additivity(out, self.model.predict(X))
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 862, in assert_additivity
    check_sum(self.expected_value[i] + phi[i].sum(-1), model_output[:, i])
    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 858, in check_sum
    raise ExplainerError(err_msg)
shap.utils._exceptions.ExplainerError: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.254216, while the model output was 0.234216. If this difference is acceptable you can set check_additivity=False to disable this check.
2025-11-10 11:30:12,797 - INFO - SHAP magyarázat elmentve.
2025-11-10 11:30:12,797 - INFO - --- Model: gradient_boosting ---
2025-11-10 11:30:12,798 - INFO - Initialize model parameters from config...
2025-11-10 11:30:12,798 - INFO - Training and evaluating...
2025-11-10 11:30:12,798 - INFO - Starting model training: GradientBoostingClassifier...
2025-11-10 11:30:50,630 - INFO - GradientBoostingClassifier training finished.
2025-11-10 11:30:50,631 - INFO - Prediction on test dataset
2025-11-10 11:30:50,658 - INFO - Metrikák számítása...
2025-11-10 11:30:50,665 - INFO - Results (gradient_boosting): {'accuracy': 0.9695, 'f1_score': 0.9637639252512067, 'pr_auc': 0.36632971755151017}
2025-11-10 11:30:50,671 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\gradient_boosting.pkl
2025-11-10 11:30:50,671 - INFO - SHAP magyarázat generálása (gradient_boosting)...
2025-11-10 11:30:50,671 - INFO - Starting SHAP explanation on model: gradient_boosting...
2025-11-10 11:30:50,681 - INFO - Calculating SHAP values...
2025-11-10 11:30:53,489 - ERROR - Hiba a SHAP magyarázat generálása során (gradient_boosting): Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 4.838153, while the model output was 4.939068. If this difference is acceptable you can set check_additivity=False to disable this check.
Traceback (most recent call last):
  File "e:\Users\G9179\source\xai_fraud_detection_benchmark\src\explain.py", line 67, in generate_shap_values
    shap_values_obj = explainer(X_to_explain)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 380, in __call__
    v = self.shap_values(X, y=y, from_call=True, check_additivity=check_additivity, approximate=approximate)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 677, in shap_values
    self.assert_additivity(out, self.model.predict(X))
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 864, in assert_additivity
    check_sum(self.expected_value + phi.sum(-1), model_output)
    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 858, in check_sum
    raise ExplainerError(err_msg)
shap.utils._exceptions.ExplainerError: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 4.838153, while the model output was 4.939068. If this difference is acceptable you can set check_additivity=False to disable this check.
2025-11-10 11:30:53,494 - INFO - SHAP magyarázat elmentve.
2025-11-10 11:30:53,494 - INFO - Benchmarking finished. Saving results...
2025-11-10 11:30:53,495 - INFO - Metrics summary saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\benchmark_summary.json
2025-11-10 11:30:53,496 - INFO - Benchmarking finished.
2025-11-10 11:35:34,323 - INFO - Strating benchmarking...
2025-11-10 11:35:34,323 - INFO - Random seed beállítva: 42
2025-11-10 11:35:34,324 - INFO - Load and preprocess data...
2025-11-10 11:35:34,325 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-10 11:35:34,729 - INFO - Nyers adat sikeresen betöltve innen: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-10 11:35:34,797 - INFO - Adatok szétválasztva. Train méret: (8000, 393), Test méret: (2000, 393)
2025-11-10 11:35:34,819 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-10 11:35:34,821 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-10 11:35:34,822 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-10 11:35:35,461 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-10 11:35:35,539 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-10 11:35:43,147 - INFO - Feldolgozott train/test szettek elmentve a(z) 'E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-10 11:35:43,162 - INFO - Successfuly loaded data. Train size: (8000, 505), Test size: (2000, 505)
2025-11-10 11:35:43,163 - INFO - Starting iteration with following models: ['logistic_regression', 'random_forest', 'gradient_boosting']
2025-11-10 11:35:43,163 - INFO - --- Model: logistic_regression ---
2025-11-10 11:35:43,163 - INFO - Initialize model parameters from config...
2025-11-10 11:35:43,163 - INFO - Training and evaluating...
2025-11-10 11:35:43,164 - INFO - Starting model training: LogisticRegression...
2025-11-10 11:35:49,631 - INFO - LogisticRegression training finished.
2025-11-10 11:35:49,631 - INFO - Prediction on test dataset
2025-11-10 11:35:49,642 - INFO - Metrikák számítása...
2025-11-10 11:35:49,648 - INFO - Results (logistic_regression): {'accuracy': 0.9645, 'f1_score': 0.9599949731165981, 'pr_auc': 0.27440597908410147}
2025-11-10 11:35:49,649 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\logistic_regression.pkl
2025-11-10 11:35:49,650 - INFO - SHAP magyarázat generálása (logistic_regression)...
2025-11-10 11:35:49,650 - INFO - Starting SHAP explanation on model: logistic_regression...
2025-11-10 11:35:49,652 - INFO - Calculating SHAP values...
2025-11-10 11:35:49,730 - INFO - SHAP calculation finished.
2025-11-10 11:35:49,730 - INFO - Generating SHAP beeswarm plot...
2025-11-10 11:35:49,996 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_beeswarm.png
2025-11-10 11:35:49,996 - INFO - Generating SHAP bar plot...
2025-11-10 11:35:50,240 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_bar.png
2025-11-10 11:35:50,242 - INFO - SHAP magyarázat elmentve.
2025-11-10 11:35:50,243 - INFO - --- Model: random_forest ---
2025-11-10 11:35:50,243 - INFO - Initialize model parameters from config...
2025-11-10 11:35:50,243 - INFO - Training and evaluating...
2025-11-10 11:35:50,244 - INFO - Starting model training: RandomForestClassifier...
2025-11-10 11:35:52,493 - INFO - RandomForestClassifier training finished.
2025-11-10 11:35:52,494 - INFO - Prediction on test dataset
2025-11-10 11:35:52,562 - INFO - Metrikák számítása...
2025-11-10 11:35:52,573 - INFO - Results (random_forest): {'accuracy': 0.9705, 'f1_score': 0.9617141505914562, 'pr_auc': 0.3807479382619139}
2025-11-10 11:35:52,580 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\random_forest.pkl
2025-11-10 11:35:52,581 - INFO - SHAP magyarázat generálása (random_forest)...
2025-11-10 11:35:52,581 - INFO - Starting SHAP explanation on model: random_forest...
2025-11-10 11:35:52,583 - WARNING - SHAP: No specific explainer for RandomForestClassifier. Using slow KernelExplainer.
2025-11-10 11:35:52,599 - INFO - Calculating SHAP values...
2025-11-10 11:35:53,309 - INFO - num_full_subsets = 0
2025-11-10 11:35:53,311 - INFO - remaining_weight_vector = array([0.15518582, 0.07781087, 0.05202003, 0.03912524, 0.03138886,
       0.02623169, 0.02254837, 0.01978619, 0.01763812, 0.01591992,
       0.01451437, 0.01334329, 0.01235258, 0.0115036 , 0.010768  ,
       0.01012451, 0.0095569 , 0.00905251, 0.00860136, 0.00819546,
       0.00782836, 0.00749477, 0.00719031, 0.00691134, 0.00665482,
       0.00641813, 0.0061991 , 0.00599582, 0.00580666, 0.00563022,
       0.00546526, 0.00531071, 0.00516563, 0.00502917, 0.0049006 ,
       0.00477927, 0.00466459, 0.00455603, 0.00445313, 0.00435545,
       0.00426262, 0.0041743 , 0.00409017, 0.00400994, 0.00393336,
       0.00386018, 0.0037902 , 0.00372321, 0.00365903, 0.00359749,
       0.00353844, 0.00348173, 0.00342724, 0.00337484, 0.00332441,
       0.00327586, 0.00322908, 0.00318398, 0.00314049, 0.00309851,
       0.00305798, 0.00301882, 0.00298097, 0.00294437, 0.00290897,
       0.00287471, 0.00284153, 0.0028094 , 0.00277826, 0.00274808,
       0.00271882, 0.00269043, 0.00266289, 0.00263615, 0.00261019,
       0.00258498, 0.00256049, 0.00253669, 0.00251356, 0.00249107,
       0.0024692 , 0.00244792, 0.00242722, 0.00240708, 0.00238747,
       0.00236839, 0.0023498 , 0.00233171, 0.00231408, 0.0022969 ,
       0.00228017, 0.00226387, 0.00224797, 0.00223248, 0.00221738,
       0.00220266, 0.00218831, 0.00217431, 0.00216065, 0.00214734,
       0.00213435, 0.00212168, 0.00210932, 0.00209726, 0.0020855 ,
       0.00207402, 0.00206283, 0.0020519 , 0.00204124, 0.00203084,
       0.00202069, 0.00201079, 0.00200113, 0.00199171, 0.00198251,
       0.00197354, 0.0019648 , 0.00195626, 0.00194794, 0.00193982,
       0.00193191, 0.00192419, 0.00191667, 0.00190934, 0.00190219,
       0.00189523, 0.00188845, 0.00188184, 0.0018754 , 0.00186914,
       0.00186304, 0.00185711, 0.00185134, 0.00184573, 0.00184027,
       0.00183497, 0.00182982, 0.00182481, 0.00181996, 0.00181525,
       0.00181068, 0.00180625, 0.00180196, 0.00179781, 0.00179379,
       0.00178991, 0.00178616, 0.00178254, 0.00177905, 0.00177568,
       0.00177245, 0.00176933, 0.00176634, 0.00176348, 0.00176073,
       0.0017581 , 0.0017556 , 0.00175321, 0.00175094, 0.00174878,
       0.00174674, 0.00174481, 0.001743  , 0.0017413 , 0.00173972,
       0.00173824, 0.00173688, 0.00173563, 0.00173449, 0.00173346,
       0.00173254, 0.00173172, 0.00173102, 0.00173043, 0.00172994,
       0.00172956, 0.00172929, 0.00172913, 0.00172908])
2025-11-10 11:35:53,313 - INFO - num_paired_subset_sizes = 178
2025-11-10 11:35:53,756 - INFO - weight_left = np.float64(0.9999999999999999)
2025-11-10 11:35:59,218 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:35:59,218 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:35:59,312 - INFO - phi = array([0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.00267267,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.00212765,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00300074, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00282847, 0.00286184, 0.        , 0.        , 0.        ,
       0.00228559, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.0020532 , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00219669, 0.00198143,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00257695, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        ])
2025-11-10 11:35:59,320 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:35:59,320 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:35:59,390 - INFO - phi = array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        , -0.00267267,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        , -0.00212765,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00300074,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00282847, -0.00286184,  0.        ,  0.        ,  0.        ,
       -0.00228559,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.0020532 ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00219669, -0.00198143,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00257695,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ])
2025-11-10 11:36:00,276 - INFO - num_full_subsets = 0
2025-11-10 11:36:00,278 - INFO - remaining_weight_vector = array([0.15518582, 0.07781087, 0.05202003, 0.03912524, 0.03138886,
       0.02623169, 0.02254837, 0.01978619, 0.01763812, 0.01591992,
       0.01451437, 0.01334329, 0.01235258, 0.0115036 , 0.010768  ,
       0.01012451, 0.0095569 , 0.00905251, 0.00860136, 0.00819546,
       0.00782836, 0.00749477, 0.00719031, 0.00691134, 0.00665482,
       0.00641813, 0.0061991 , 0.00599582, 0.00580666, 0.00563022,
       0.00546526, 0.00531071, 0.00516563, 0.00502917, 0.0049006 ,
       0.00477927, 0.00466459, 0.00455603, 0.00445313, 0.00435545,
       0.00426262, 0.0041743 , 0.00409017, 0.00400994, 0.00393336,
       0.00386018, 0.0037902 , 0.00372321, 0.00365903, 0.00359749,
       0.00353844, 0.00348173, 0.00342724, 0.00337484, 0.00332441,
       0.00327586, 0.00322908, 0.00318398, 0.00314049, 0.00309851,
       0.00305798, 0.00301882, 0.00298097, 0.00294437, 0.00290897,
       0.00287471, 0.00284153, 0.0028094 , 0.00277826, 0.00274808,
       0.00271882, 0.00269043, 0.00266289, 0.00263615, 0.00261019,
       0.00258498, 0.00256049, 0.00253669, 0.00251356, 0.00249107,
       0.0024692 , 0.00244792, 0.00242722, 0.00240708, 0.00238747,
       0.00236839, 0.0023498 , 0.00233171, 0.00231408, 0.0022969 ,
       0.00228017, 0.00226387, 0.00224797, 0.00223248, 0.00221738,
       0.00220266, 0.00218831, 0.00217431, 0.00216065, 0.00214734,
       0.00213435, 0.00212168, 0.00210932, 0.00209726, 0.0020855 ,
       0.00207402, 0.00206283, 0.0020519 , 0.00204124, 0.00203084,
       0.00202069, 0.00201079, 0.00200113, 0.00199171, 0.00198251,
       0.00197354, 0.0019648 , 0.00195626, 0.00194794, 0.00193982,
       0.00193191, 0.00192419, 0.00191667, 0.00190934, 0.00190219,
       0.00189523, 0.00188845, 0.00188184, 0.0018754 , 0.00186914,
       0.00186304, 0.00185711, 0.00185134, 0.00184573, 0.00184027,
       0.00183497, 0.00182982, 0.00182481, 0.00181996, 0.00181525,
       0.00181068, 0.00180625, 0.00180196, 0.00179781, 0.00179379,
       0.00178991, 0.00178616, 0.00178254, 0.00177905, 0.00177568,
       0.00177245, 0.00176933, 0.00176634, 0.00176348, 0.00176073,
       0.0017581 , 0.0017556 , 0.00175321, 0.00175094, 0.00174878,
       0.00174674, 0.00174481, 0.001743  , 0.0017413 , 0.00173972,
       0.00173824, 0.00173688, 0.00173563, 0.00173449, 0.00173346,
       0.00173254, 0.00173172, 0.00173102, 0.00173043, 0.00172994,
       0.00172956, 0.00172929, 0.00172913, 0.00172908])
2025-11-10 11:36:00,280 - INFO - num_paired_subset_sizes = 178
2025-11-10 11:36:00,722 - INFO - weight_left = np.float64(0.9999999999999999)
2025-11-10 11:36:06,554 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:36:06,555 - INFO - np.sum(self.kernelWeights) = np.float64(0.9999999999999997)
2025-11-10 11:36:06,636 - INFO - phi = array([0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00219404, 0.00265656,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00265442, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00271659, 0.00247191, 0.        , 0.        , 0.        ,
       0.00193149, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00191414, 0.00206445, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00238608, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.00213278, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        ])
2025-11-10 11:36:06,644 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:36:06,645 - INFO - np.sum(self.kernelWeights) = np.float64(0.9999999999999997)
2025-11-10 11:36:06,717 - INFO - phi = array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00219404, -0.00265656,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00265442,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00271659, -0.00247191,  0.        ,  0.        ,  0.        ,
       -0.00193149,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00191414, -0.00206445,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00238608,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.00213278,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ])
2025-11-10 11:36:07,549 - INFO - num_full_subsets = 0
2025-11-10 11:36:07,551 - INFO - remaining_weight_vector = array([0.15518582, 0.07781087, 0.05202003, 0.03912524, 0.03138886,
       0.02623169, 0.02254837, 0.01978619, 0.01763812, 0.01591992,
       0.01451437, 0.01334329, 0.01235258, 0.0115036 , 0.010768  ,
       0.01012451, 0.0095569 , 0.00905251, 0.00860136, 0.00819546,
       0.00782836, 0.00749477, 0.00719031, 0.00691134, 0.00665482,
       0.00641813, 0.0061991 , 0.00599582, 0.00580666, 0.00563022,
       0.00546526, 0.00531071, 0.00516563, 0.00502917, 0.0049006 ,
       0.00477927, 0.00466459, 0.00455603, 0.00445313, 0.00435545,
       0.00426262, 0.0041743 , 0.00409017, 0.00400994, 0.00393336,
       0.00386018, 0.0037902 , 0.00372321, 0.00365903, 0.00359749,
       0.00353844, 0.00348173, 0.00342724, 0.00337484, 0.00332441,
       0.00327586, 0.00322908, 0.00318398, 0.00314049, 0.00309851,
       0.00305798, 0.00301882, 0.00298097, 0.00294437, 0.00290897,
       0.00287471, 0.00284153, 0.0028094 , 0.00277826, 0.00274808,
       0.00271882, 0.00269043, 0.00266289, 0.00263615, 0.00261019,
       0.00258498, 0.00256049, 0.00253669, 0.00251356, 0.00249107,
       0.0024692 , 0.00244792, 0.00242722, 0.00240708, 0.00238747,
       0.00236839, 0.0023498 , 0.00233171, 0.00231408, 0.0022969 ,
       0.00228017, 0.00226387, 0.00224797, 0.00223248, 0.00221738,
       0.00220266, 0.00218831, 0.00217431, 0.00216065, 0.00214734,
       0.00213435, 0.00212168, 0.00210932, 0.00209726, 0.0020855 ,
       0.00207402, 0.00206283, 0.0020519 , 0.00204124, 0.00203084,
       0.00202069, 0.00201079, 0.00200113, 0.00199171, 0.00198251,
       0.00197354, 0.0019648 , 0.00195626, 0.00194794, 0.00193982,
       0.00193191, 0.00192419, 0.00191667, 0.00190934, 0.00190219,
       0.00189523, 0.00188845, 0.00188184, 0.0018754 , 0.00186914,
       0.00186304, 0.00185711, 0.00185134, 0.00184573, 0.00184027,
       0.00183497, 0.00182982, 0.00182481, 0.00181996, 0.00181525,
       0.00181068, 0.00180625, 0.00180196, 0.00179781, 0.00179379,
       0.00178991, 0.00178616, 0.00178254, 0.00177905, 0.00177568,
       0.00177245, 0.00176933, 0.00176634, 0.00176348, 0.00176073,
       0.0017581 , 0.0017556 , 0.00175321, 0.00175094, 0.00174878,
       0.00174674, 0.00174481, 0.001743  , 0.0017413 , 0.00173972,
       0.00173824, 0.00173688, 0.00173563, 0.00173449, 0.00173346,
       0.00173254, 0.00173172, 0.00173102, 0.00173043, 0.00172994,
       0.00172956, 0.00172929, 0.00172913, 0.00172908])
2025-11-10 11:36:07,553 - INFO - num_paired_subset_sizes = 178
2025-11-10 11:36:08,028 - INFO - weight_left = np.float64(0.9999999999999999)
2025-11-10 11:36:13,733 - INFO - np.sum(w_aug) = np.float64(357.9999999999999)
2025-11-10 11:36:13,733 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:36:13,809 - INFO - phi = array([0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.00098744, 0.        , 0.        , 0.        ,
       0.        , 0.00065524, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00080992, 0.00144021,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.00204133, 0.00179196, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.00090996,
       0.        , 0.        , 0.        , 0.        , 0.00130165,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00070891, 0.00098143, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        ])
2025-11-10 11:36:13,817 - INFO - np.sum(w_aug) = np.float64(357.9999999999999)
2025-11-10 11:36:13,817 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:36:13,884 - INFO - phi = array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        , -0.00098744,  0.        ,  0.        ,  0.        ,
        0.        , -0.00065524,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00080992, -0.00144021,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.00204133, -0.00179196,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        , -0.00090996,
        0.        ,  0.        ,  0.        ,  0.        , -0.00130165,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00070891, -0.00098143,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ])
2025-11-10 11:36:14,705 - INFO - num_full_subsets = 0
2025-11-10 11:36:14,707 - INFO - remaining_weight_vector = array([0.15518582, 0.07781087, 0.05202003, 0.03912524, 0.03138886,
       0.02623169, 0.02254837, 0.01978619, 0.01763812, 0.01591992,
       0.01451437, 0.01334329, 0.01235258, 0.0115036 , 0.010768  ,
       0.01012451, 0.0095569 , 0.00905251, 0.00860136, 0.00819546,
       0.00782836, 0.00749477, 0.00719031, 0.00691134, 0.00665482,
       0.00641813, 0.0061991 , 0.00599582, 0.00580666, 0.00563022,
       0.00546526, 0.00531071, 0.00516563, 0.00502917, 0.0049006 ,
       0.00477927, 0.00466459, 0.00455603, 0.00445313, 0.00435545,
       0.00426262, 0.0041743 , 0.00409017, 0.00400994, 0.00393336,
       0.00386018, 0.0037902 , 0.00372321, 0.00365903, 0.00359749,
       0.00353844, 0.00348173, 0.00342724, 0.00337484, 0.00332441,
       0.00327586, 0.00322908, 0.00318398, 0.00314049, 0.00309851,
       0.00305798, 0.00301882, 0.00298097, 0.00294437, 0.00290897,
       0.00287471, 0.00284153, 0.0028094 , 0.00277826, 0.00274808,
       0.00271882, 0.00269043, 0.00266289, 0.00263615, 0.00261019,
       0.00258498, 0.00256049, 0.00253669, 0.00251356, 0.00249107,
       0.0024692 , 0.00244792, 0.00242722, 0.00240708, 0.00238747,
       0.00236839, 0.0023498 , 0.00233171, 0.00231408, 0.0022969 ,
       0.00228017, 0.00226387, 0.00224797, 0.00223248, 0.00221738,
       0.00220266, 0.00218831, 0.00217431, 0.00216065, 0.00214734,
       0.00213435, 0.00212168, 0.00210932, 0.00209726, 0.0020855 ,
       0.00207402, 0.00206283, 0.0020519 , 0.00204124, 0.00203084,
       0.00202069, 0.00201079, 0.00200113, 0.00199171, 0.00198251,
       0.00197354, 0.0019648 , 0.00195626, 0.00194794, 0.00193982,
       0.00193191, 0.00192419, 0.00191667, 0.00190934, 0.00190219,
       0.00189523, 0.00188845, 0.00188184, 0.0018754 , 0.00186914,
       0.00186304, 0.00185711, 0.00185134, 0.00184573, 0.00184027,
       0.00183497, 0.00182982, 0.00182481, 0.00181996, 0.00181525,
       0.00181068, 0.00180625, 0.00180196, 0.00179781, 0.00179379,
       0.00178991, 0.00178616, 0.00178254, 0.00177905, 0.00177568,
       0.00177245, 0.00176933, 0.00176634, 0.00176348, 0.00176073,
       0.0017581 , 0.0017556 , 0.00175321, 0.00175094, 0.00174878,
       0.00174674, 0.00174481, 0.001743  , 0.0017413 , 0.00173972,
       0.00173824, 0.00173688, 0.00173563, 0.00173449, 0.00173346,
       0.00173254, 0.00173172, 0.00173102, 0.00173043, 0.00172994,
       0.00172956, 0.00172929, 0.00172913, 0.00172908])
2025-11-10 11:36:14,709 - INFO - num_paired_subset_sizes = 178
2025-11-10 11:36:15,151 - INFO - weight_left = np.float64(0.9999999999999999)
2025-11-10 11:36:20,810 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:36:20,811 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:36:20,886 - INFO - phi = array([0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00236324, 0.00336185,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00329156, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00263329, 0.00267236, 0.        , 0.        , 0.        ,
       0.00251041, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.00219407, 0.00194102, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.00205905,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.00196072,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        ])
2025-11-10 11:36:20,893 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:36:20,894 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:36:20,964 - INFO - phi = array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00236324, -0.00336185,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00329156,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00263329, -0.00267236,  0.        ,  0.        ,  0.        ,
       -0.00251041,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.00219407, -0.00194102,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        , -0.00205905,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        , -0.00196072,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ])
2025-11-10 11:36:21,889 - INFO - num_full_subsets = 0
2025-11-10 11:36:21,890 - INFO - remaining_weight_vector = array([0.15518582, 0.07781087, 0.05202003, 0.03912524, 0.03138886,
       0.02623169, 0.02254837, 0.01978619, 0.01763812, 0.01591992,
       0.01451437, 0.01334329, 0.01235258, 0.0115036 , 0.010768  ,
       0.01012451, 0.0095569 , 0.00905251, 0.00860136, 0.00819546,
       0.00782836, 0.00749477, 0.00719031, 0.00691134, 0.00665482,
       0.00641813, 0.0061991 , 0.00599582, 0.00580666, 0.00563022,
       0.00546526, 0.00531071, 0.00516563, 0.00502917, 0.0049006 ,
       0.00477927, 0.00466459, 0.00455603, 0.00445313, 0.00435545,
       0.00426262, 0.0041743 , 0.00409017, 0.00400994, 0.00393336,
       0.00386018, 0.0037902 , 0.00372321, 0.00365903, 0.00359749,
       0.00353844, 0.00348173, 0.00342724, 0.00337484, 0.00332441,
       0.00327586, 0.00322908, 0.00318398, 0.00314049, 0.00309851,
       0.00305798, 0.00301882, 0.00298097, 0.00294437, 0.00290897,
       0.00287471, 0.00284153, 0.0028094 , 0.00277826, 0.00274808,
       0.00271882, 0.00269043, 0.00266289, 0.00263615, 0.00261019,
       0.00258498, 0.00256049, 0.00253669, 0.00251356, 0.00249107,
       0.0024692 , 0.00244792, 0.00242722, 0.00240708, 0.00238747,
       0.00236839, 0.0023498 , 0.00233171, 0.00231408, 0.0022969 ,
       0.00228017, 0.00226387, 0.00224797, 0.00223248, 0.00221738,
       0.00220266, 0.00218831, 0.00217431, 0.00216065, 0.00214734,
       0.00213435, 0.00212168, 0.00210932, 0.00209726, 0.0020855 ,
       0.00207402, 0.00206283, 0.0020519 , 0.00204124, 0.00203084,
       0.00202069, 0.00201079, 0.00200113, 0.00199171, 0.00198251,
       0.00197354, 0.0019648 , 0.00195626, 0.00194794, 0.00193982,
       0.00193191, 0.00192419, 0.00191667, 0.00190934, 0.00190219,
       0.00189523, 0.00188845, 0.00188184, 0.0018754 , 0.00186914,
       0.00186304, 0.00185711, 0.00185134, 0.00184573, 0.00184027,
       0.00183497, 0.00182982, 0.00182481, 0.00181996, 0.00181525,
       0.00181068, 0.00180625, 0.00180196, 0.00179781, 0.00179379,
       0.00178991, 0.00178616, 0.00178254, 0.00177905, 0.00177568,
       0.00177245, 0.00176933, 0.00176634, 0.00176348, 0.00176073,
       0.0017581 , 0.0017556 , 0.00175321, 0.00175094, 0.00174878,
       0.00174674, 0.00174481, 0.001743  , 0.0017413 , 0.00173972,
       0.00173824, 0.00173688, 0.00173563, 0.00173449, 0.00173346,
       0.00173254, 0.00173172, 0.00173102, 0.00173043, 0.00172994,
       0.00172956, 0.00172929, 0.00172913, 0.00172908])
2025-11-10 11:36:21,892 - INFO - num_paired_subset_sizes = 178
2025-11-10 11:36:22,337 - INFO - weight_left = np.float64(0.9999999999999999)
2025-11-10 11:36:28,010 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:36:28,010 - INFO - np.sum(self.kernelWeights) = np.float64(0.9999999999999998)
2025-11-10 11:36:28,090 - INFO - phi = array([0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00125292, 0.00313531,
       0.        , 0.00128361, 0.        , 0.        , 0.        ,
       0.00044729, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00159788, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.00079282,
       0.        , 0.        , 0.        , 0.        , 0.00106227,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00093729, 0.00103261, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00040441, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        ])
2025-11-10 11:36:28,097 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:36:28,098 - INFO - np.sum(self.kernelWeights) = np.float64(0.9999999999999998)
2025-11-10 11:36:28,170 - INFO - phi = array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00125292, -0.00313531,
        0.        , -0.00128361,  0.        ,  0.        ,  0.        ,
       -0.00044729,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00159788,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        , -0.00079282,
        0.        ,  0.        ,  0.        ,  0.        , -0.00106227,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00093729, -0.00103261,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00040441,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ])
2025-11-10 11:36:29,001 - INFO - num_full_subsets = 0
2025-11-10 11:36:29,003 - INFO - remaining_weight_vector = array([0.15518582, 0.07781087, 0.05202003, 0.03912524, 0.03138886,
       0.02623169, 0.02254837, 0.01978619, 0.01763812, 0.01591992,
       0.01451437, 0.01334329, 0.01235258, 0.0115036 , 0.010768  ,
       0.01012451, 0.0095569 , 0.00905251, 0.00860136, 0.00819546,
       0.00782836, 0.00749477, 0.00719031, 0.00691134, 0.00665482,
       0.00641813, 0.0061991 , 0.00599582, 0.00580666, 0.00563022,
       0.00546526, 0.00531071, 0.00516563, 0.00502917, 0.0049006 ,
       0.00477927, 0.00466459, 0.00455603, 0.00445313, 0.00435545,
       0.00426262, 0.0041743 , 0.00409017, 0.00400994, 0.00393336,
       0.00386018, 0.0037902 , 0.00372321, 0.00365903, 0.00359749,
       0.00353844, 0.00348173, 0.00342724, 0.00337484, 0.00332441,
       0.00327586, 0.00322908, 0.00318398, 0.00314049, 0.00309851,
       0.00305798, 0.00301882, 0.00298097, 0.00294437, 0.00290897,
       0.00287471, 0.00284153, 0.0028094 , 0.00277826, 0.00274808,
       0.00271882, 0.00269043, 0.00266289, 0.00263615, 0.00261019,
       0.00258498, 0.00256049, 0.00253669, 0.00251356, 0.00249107,
       0.0024692 , 0.00244792, 0.00242722, 0.00240708, 0.00238747,
       0.00236839, 0.0023498 , 0.00233171, 0.00231408, 0.0022969 ,
       0.00228017, 0.00226387, 0.00224797, 0.00223248, 0.00221738,
       0.00220266, 0.00218831, 0.00217431, 0.00216065, 0.00214734,
       0.00213435, 0.00212168, 0.00210932, 0.00209726, 0.0020855 ,
       0.00207402, 0.00206283, 0.0020519 , 0.00204124, 0.00203084,
       0.00202069, 0.00201079, 0.00200113, 0.00199171, 0.00198251,
       0.00197354, 0.0019648 , 0.00195626, 0.00194794, 0.00193982,
       0.00193191, 0.00192419, 0.00191667, 0.00190934, 0.00190219,
       0.00189523, 0.00188845, 0.00188184, 0.0018754 , 0.00186914,
       0.00186304, 0.00185711, 0.00185134, 0.00184573, 0.00184027,
       0.00183497, 0.00182982, 0.00182481, 0.00181996, 0.00181525,
       0.00181068, 0.00180625, 0.00180196, 0.00179781, 0.00179379,
       0.00178991, 0.00178616, 0.00178254, 0.00177905, 0.00177568,
       0.00177245, 0.00176933, 0.00176634, 0.00176348, 0.00176073,
       0.0017581 , 0.0017556 , 0.00175321, 0.00175094, 0.00174878,
       0.00174674, 0.00174481, 0.001743  , 0.0017413 , 0.00173972,
       0.00173824, 0.00173688, 0.00173563, 0.00173449, 0.00173346,
       0.00173254, 0.00173172, 0.00173102, 0.00173043, 0.00172994,
       0.00172956, 0.00172929, 0.00172913, 0.00172908])
2025-11-10 11:36:29,004 - INFO - num_paired_subset_sizes = 178
2025-11-10 11:36:29,446 - INFO - weight_left = np.float64(0.9999999999999999)
2025-11-10 11:36:34,992 - INFO - np.sum(w_aug) = np.float64(357.99999999999994)
2025-11-10 11:36:34,992 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:36:35,068 - INFO - phi = array([0.        , 0.        , 0.00114755, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.00129363, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00059962, 0.00181153,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00248054, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.00126328,
       0.        , 0.        , 0.        , 0.        , 0.00207888,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.00092334, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.00061165, 0.        , 0.        ,
       0.        , 0.        , 0.00060662, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        ])
2025-11-10 11:36:35,076 - INFO - np.sum(w_aug) = np.float64(357.99999999999994)
2025-11-10 11:36:35,076 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:36:35,146 - INFO - phi = array([ 0.        ,  0.        , -0.00114755,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        , -0.00129363,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00059962, -0.00181153,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00248054,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        , -0.00126328,
        0.        ,  0.        ,  0.        ,  0.        , -0.00207888,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        , -0.00092334,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.00061165,  0.        ,  0.        ,
        0.        ,  0.        , -0.00060662,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ])
2025-11-10 11:36:35,970 - INFO - num_full_subsets = 0
2025-11-10 11:36:35,972 - INFO - remaining_weight_vector = array([0.15518582, 0.07781087, 0.05202003, 0.03912524, 0.03138886,
       0.02623169, 0.02254837, 0.01978619, 0.01763812, 0.01591992,
       0.01451437, 0.01334329, 0.01235258, 0.0115036 , 0.010768  ,
       0.01012451, 0.0095569 , 0.00905251, 0.00860136, 0.00819546,
       0.00782836, 0.00749477, 0.00719031, 0.00691134, 0.00665482,
       0.00641813, 0.0061991 , 0.00599582, 0.00580666, 0.00563022,
       0.00546526, 0.00531071, 0.00516563, 0.00502917, 0.0049006 ,
       0.00477927, 0.00466459, 0.00455603, 0.00445313, 0.00435545,
       0.00426262, 0.0041743 , 0.00409017, 0.00400994, 0.00393336,
       0.00386018, 0.0037902 , 0.00372321, 0.00365903, 0.00359749,
       0.00353844, 0.00348173, 0.00342724, 0.00337484, 0.00332441,
       0.00327586, 0.00322908, 0.00318398, 0.00314049, 0.00309851,
       0.00305798, 0.00301882, 0.00298097, 0.00294437, 0.00290897,
       0.00287471, 0.00284153, 0.0028094 , 0.00277826, 0.00274808,
       0.00271882, 0.00269043, 0.00266289, 0.00263615, 0.00261019,
       0.00258498, 0.00256049, 0.00253669, 0.00251356, 0.00249107,
       0.0024692 , 0.00244792, 0.00242722, 0.00240708, 0.00238747,
       0.00236839, 0.0023498 , 0.00233171, 0.00231408, 0.0022969 ,
       0.00228017, 0.00226387, 0.00224797, 0.00223248, 0.00221738,
       0.00220266, 0.00218831, 0.00217431, 0.00216065, 0.00214734,
       0.00213435, 0.00212168, 0.00210932, 0.00209726, 0.0020855 ,
       0.00207402, 0.00206283, 0.0020519 , 0.00204124, 0.00203084,
       0.00202069, 0.00201079, 0.00200113, 0.00199171, 0.00198251,
       0.00197354, 0.0019648 , 0.00195626, 0.00194794, 0.00193982,
       0.00193191, 0.00192419, 0.00191667, 0.00190934, 0.00190219,
       0.00189523, 0.00188845, 0.00188184, 0.0018754 , 0.00186914,
       0.00186304, 0.00185711, 0.00185134, 0.00184573, 0.00184027,
       0.00183497, 0.00182982, 0.00182481, 0.00181996, 0.00181525,
       0.00181068, 0.00180625, 0.00180196, 0.00179781, 0.00179379,
       0.00178991, 0.00178616, 0.00178254, 0.00177905, 0.00177568,
       0.00177245, 0.00176933, 0.00176634, 0.00176348, 0.00176073,
       0.0017581 , 0.0017556 , 0.00175321, 0.00175094, 0.00174878,
       0.00174674, 0.00174481, 0.001743  , 0.0017413 , 0.00173972,
       0.00173824, 0.00173688, 0.00173563, 0.00173449, 0.00173346,
       0.00173254, 0.00173172, 0.00173102, 0.00173043, 0.00172994,
       0.00172956, 0.00172929, 0.00172913, 0.00172908])
2025-11-10 11:36:35,974 - INFO - num_paired_subset_sizes = 178
2025-11-10 11:36:36,409 - INFO - weight_left = np.float64(0.9999999999999999)
2025-11-10 11:36:41,985 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:36:41,985 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:36:42,064 - INFO - phi = array([0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00182509, 0.00282955,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00246809, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00270639, 0.00248687, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.00220617, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.00213188,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00214089, 0.00194985, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00219922, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        ])
2025-11-10 11:36:42,080 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:36:42,081 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:36:42,156 - INFO - phi = array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00182509, -0.00282955,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00246809,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00270639, -0.00248687,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        , -0.00220617,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        , -0.00213188,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00214089, -0.00194985,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00219922,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ])
2025-11-10 11:36:42,982 - INFO - num_full_subsets = 0
2025-11-10 11:36:42,983 - INFO - remaining_weight_vector = array([0.15518582, 0.07781087, 0.05202003, 0.03912524, 0.03138886,
       0.02623169, 0.02254837, 0.01978619, 0.01763812, 0.01591992,
       0.01451437, 0.01334329, 0.01235258, 0.0115036 , 0.010768  ,
       0.01012451, 0.0095569 , 0.00905251, 0.00860136, 0.00819546,
       0.00782836, 0.00749477, 0.00719031, 0.00691134, 0.00665482,
       0.00641813, 0.0061991 , 0.00599582, 0.00580666, 0.00563022,
       0.00546526, 0.00531071, 0.00516563, 0.00502917, 0.0049006 ,
       0.00477927, 0.00466459, 0.00455603, 0.00445313, 0.00435545,
       0.00426262, 0.0041743 , 0.00409017, 0.00400994, 0.00393336,
       0.00386018, 0.0037902 , 0.00372321, 0.00365903, 0.00359749,
       0.00353844, 0.00348173, 0.00342724, 0.00337484, 0.00332441,
       0.00327586, 0.00322908, 0.00318398, 0.00314049, 0.00309851,
       0.00305798, 0.00301882, 0.00298097, 0.00294437, 0.00290897,
       0.00287471, 0.00284153, 0.0028094 , 0.00277826, 0.00274808,
       0.00271882, 0.00269043, 0.00266289, 0.00263615, 0.00261019,
       0.00258498, 0.00256049, 0.00253669, 0.00251356, 0.00249107,
       0.0024692 , 0.00244792, 0.00242722, 0.00240708, 0.00238747,
       0.00236839, 0.0023498 , 0.00233171, 0.00231408, 0.0022969 ,
       0.00228017, 0.00226387, 0.00224797, 0.00223248, 0.00221738,
       0.00220266, 0.00218831, 0.00217431, 0.00216065, 0.00214734,
       0.00213435, 0.00212168, 0.00210932, 0.00209726, 0.0020855 ,
       0.00207402, 0.00206283, 0.0020519 , 0.00204124, 0.00203084,
       0.00202069, 0.00201079, 0.00200113, 0.00199171, 0.00198251,
       0.00197354, 0.0019648 , 0.00195626, 0.00194794, 0.00193982,
       0.00193191, 0.00192419, 0.00191667, 0.00190934, 0.00190219,
       0.00189523, 0.00188845, 0.00188184, 0.0018754 , 0.00186914,
       0.00186304, 0.00185711, 0.00185134, 0.00184573, 0.00184027,
       0.00183497, 0.00182982, 0.00182481, 0.00181996, 0.00181525,
       0.00181068, 0.00180625, 0.00180196, 0.00179781, 0.00179379,
       0.00178991, 0.00178616, 0.00178254, 0.00177905, 0.00177568,
       0.00177245, 0.00176933, 0.00176634, 0.00176348, 0.00176073,
       0.0017581 , 0.0017556 , 0.00175321, 0.00175094, 0.00174878,
       0.00174674, 0.00174481, 0.001743  , 0.0017413 , 0.00173972,
       0.00173824, 0.00173688, 0.00173563, 0.00173449, 0.00173346,
       0.00173254, 0.00173172, 0.00173102, 0.00173043, 0.00172994,
       0.00172956, 0.00172929, 0.00172913, 0.00172908])
2025-11-10 11:36:42,985 - INFO - num_paired_subset_sizes = 178
2025-11-10 11:36:43,496 - INFO - weight_left = np.float64(0.9999999999999999)
2025-11-10 11:36:49,162 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:36:49,162 - INFO - np.sum(self.kernelWeights) = np.float64(0.9999999999999998)
2025-11-10 11:36:49,244 - INFO - phi = array([0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.00148302, 0.        , 0.00198194,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00238607, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00212306, 0.00164796, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00176124, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.00144643,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.0012623 , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.00163245, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.0020365 , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        ])
2025-11-10 11:36:49,252 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:36:49,252 - INFO - np.sum(self.kernelWeights) = np.float64(0.9999999999999998)
2025-11-10 11:36:49,328 - INFO - phi = array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.00148302,  0.        , -0.00198194,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00238607,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00212306, -0.00164796,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00176124,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        , -0.00144643,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.0012623 ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.00163245,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.0020365 ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ])
2025-11-10 11:36:50,213 - INFO - num_full_subsets = 0
2025-11-10 11:36:50,214 - INFO - remaining_weight_vector = array([0.15518582, 0.07781087, 0.05202003, 0.03912524, 0.03138886,
       0.02623169, 0.02254837, 0.01978619, 0.01763812, 0.01591992,
       0.01451437, 0.01334329, 0.01235258, 0.0115036 , 0.010768  ,
       0.01012451, 0.0095569 , 0.00905251, 0.00860136, 0.00819546,
       0.00782836, 0.00749477, 0.00719031, 0.00691134, 0.00665482,
       0.00641813, 0.0061991 , 0.00599582, 0.00580666, 0.00563022,
       0.00546526, 0.00531071, 0.00516563, 0.00502917, 0.0049006 ,
       0.00477927, 0.00466459, 0.00455603, 0.00445313, 0.00435545,
       0.00426262, 0.0041743 , 0.00409017, 0.00400994, 0.00393336,
       0.00386018, 0.0037902 , 0.00372321, 0.00365903, 0.00359749,
       0.00353844, 0.00348173, 0.00342724, 0.00337484, 0.00332441,
       0.00327586, 0.00322908, 0.00318398, 0.00314049, 0.00309851,
       0.00305798, 0.00301882, 0.00298097, 0.00294437, 0.00290897,
       0.00287471, 0.00284153, 0.0028094 , 0.00277826, 0.00274808,
       0.00271882, 0.00269043, 0.00266289, 0.00263615, 0.00261019,
       0.00258498, 0.00256049, 0.00253669, 0.00251356, 0.00249107,
       0.0024692 , 0.00244792, 0.00242722, 0.00240708, 0.00238747,
       0.00236839, 0.0023498 , 0.00233171, 0.00231408, 0.0022969 ,
       0.00228017, 0.00226387, 0.00224797, 0.00223248, 0.00221738,
       0.00220266, 0.00218831, 0.00217431, 0.00216065, 0.00214734,
       0.00213435, 0.00212168, 0.00210932, 0.00209726, 0.0020855 ,
       0.00207402, 0.00206283, 0.0020519 , 0.00204124, 0.00203084,
       0.00202069, 0.00201079, 0.00200113, 0.00199171, 0.00198251,
       0.00197354, 0.0019648 , 0.00195626, 0.00194794, 0.00193982,
       0.00193191, 0.00192419, 0.00191667, 0.00190934, 0.00190219,
       0.00189523, 0.00188845, 0.00188184, 0.0018754 , 0.00186914,
       0.00186304, 0.00185711, 0.00185134, 0.00184573, 0.00184027,
       0.00183497, 0.00182982, 0.00182481, 0.00181996, 0.00181525,
       0.00181068, 0.00180625, 0.00180196, 0.00179781, 0.00179379,
       0.00178991, 0.00178616, 0.00178254, 0.00177905, 0.00177568,
       0.00177245, 0.00176933, 0.00176634, 0.00176348, 0.00176073,
       0.0017581 , 0.0017556 , 0.00175321, 0.00175094, 0.00174878,
       0.00174674, 0.00174481, 0.001743  , 0.0017413 , 0.00173972,
       0.00173824, 0.00173688, 0.00173563, 0.00173449, 0.00173346,
       0.00173254, 0.00173172, 0.00173102, 0.00173043, 0.00172994,
       0.00172956, 0.00172929, 0.00172913, 0.00172908])
2025-11-10 11:36:50,216 - INFO - num_paired_subset_sizes = 178
2025-11-10 11:36:50,658 - INFO - weight_left = np.float64(0.9999999999999999)
2025-11-10 11:36:56,070 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:36:56,070 - INFO - np.sum(self.kernelWeights) = np.float64(0.9999999999999998)
2025-11-10 11:36:56,146 - INFO - phi = array([0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00165868, 0.        ,
       0.        , 0.        , 0.        , 0.00233966, 0.00248755,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00197229, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.0022537 , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00254387, 0.00279104, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.00199386,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00184693, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.0020285 , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        ])
2025-11-10 11:36:56,153 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:36:56,154 - INFO - np.sum(self.kernelWeights) = np.float64(0.9999999999999998)
2025-11-10 11:36:56,224 - INFO - phi = array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00165868,  0.        ,
        0.        ,  0.        ,  0.        , -0.00233966, -0.00248755,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00197229,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.0022537 ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00254387, -0.00279104,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        , -0.00199386,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00184693,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.0020285 ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ])
2025-11-10 11:36:57,054 - INFO - num_full_subsets = 0
2025-11-10 11:36:57,056 - INFO - remaining_weight_vector = array([0.15518582, 0.07781087, 0.05202003, 0.03912524, 0.03138886,
       0.02623169, 0.02254837, 0.01978619, 0.01763812, 0.01591992,
       0.01451437, 0.01334329, 0.01235258, 0.0115036 , 0.010768  ,
       0.01012451, 0.0095569 , 0.00905251, 0.00860136, 0.00819546,
       0.00782836, 0.00749477, 0.00719031, 0.00691134, 0.00665482,
       0.00641813, 0.0061991 , 0.00599582, 0.00580666, 0.00563022,
       0.00546526, 0.00531071, 0.00516563, 0.00502917, 0.0049006 ,
       0.00477927, 0.00466459, 0.00455603, 0.00445313, 0.00435545,
       0.00426262, 0.0041743 , 0.00409017, 0.00400994, 0.00393336,
       0.00386018, 0.0037902 , 0.00372321, 0.00365903, 0.00359749,
       0.00353844, 0.00348173, 0.00342724, 0.00337484, 0.00332441,
       0.00327586, 0.00322908, 0.00318398, 0.00314049, 0.00309851,
       0.00305798, 0.00301882, 0.00298097, 0.00294437, 0.00290897,
       0.00287471, 0.00284153, 0.0028094 , 0.00277826, 0.00274808,
       0.00271882, 0.00269043, 0.00266289, 0.00263615, 0.00261019,
       0.00258498, 0.00256049, 0.00253669, 0.00251356, 0.00249107,
       0.0024692 , 0.00244792, 0.00242722, 0.00240708, 0.00238747,
       0.00236839, 0.0023498 , 0.00233171, 0.00231408, 0.0022969 ,
       0.00228017, 0.00226387, 0.00224797, 0.00223248, 0.00221738,
       0.00220266, 0.00218831, 0.00217431, 0.00216065, 0.00214734,
       0.00213435, 0.00212168, 0.00210932, 0.00209726, 0.0020855 ,
       0.00207402, 0.00206283, 0.0020519 , 0.00204124, 0.00203084,
       0.00202069, 0.00201079, 0.00200113, 0.00199171, 0.00198251,
       0.00197354, 0.0019648 , 0.00195626, 0.00194794, 0.00193982,
       0.00193191, 0.00192419, 0.00191667, 0.00190934, 0.00190219,
       0.00189523, 0.00188845, 0.00188184, 0.0018754 , 0.00186914,
       0.00186304, 0.00185711, 0.00185134, 0.00184573, 0.00184027,
       0.00183497, 0.00182982, 0.00182481, 0.00181996, 0.00181525,
       0.00181068, 0.00180625, 0.00180196, 0.00179781, 0.00179379,
       0.00178991, 0.00178616, 0.00178254, 0.00177905, 0.00177568,
       0.00177245, 0.00176933, 0.00176634, 0.00176348, 0.00176073,
       0.0017581 , 0.0017556 , 0.00175321, 0.00175094, 0.00174878,
       0.00174674, 0.00174481, 0.001743  , 0.0017413 , 0.00173972,
       0.00173824, 0.00173688, 0.00173563, 0.00173449, 0.00173346,
       0.00173254, 0.00173172, 0.00173102, 0.00173043, 0.00172994,
       0.00172956, 0.00172929, 0.00172913, 0.00172908])
2025-11-10 11:36:57,058 - INFO - num_paired_subset_sizes = 178
2025-11-10 11:36:57,536 - INFO - weight_left = np.float64(0.9999999999999999)
2025-11-10 11:37:03,141 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:37:03,142 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:37:03,222 - INFO - phi = array([0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.0019424 , 0.00216303, 0.00354274,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00280498, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00288304, 0.00246122, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00205196, 0.00210175,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.00179198, 0.        , 0.        ,
       0.        , 0.        , 0.00177268, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        ])
2025-11-10 11:37:03,230 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:37:03,231 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:37:03,303 - INFO - phi = array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.0019424 , -0.00216303, -0.00354274,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00280498,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00288304, -0.00246122,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00205196, -0.00210175,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.00179198,  0.        ,  0.        ,
        0.        ,  0.        , -0.00177268,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ])
2025-11-10 11:37:04,185 - INFO - num_full_subsets = 0
2025-11-10 11:37:04,187 - INFO - remaining_weight_vector = array([0.15518582, 0.07781087, 0.05202003, 0.03912524, 0.03138886,
       0.02623169, 0.02254837, 0.01978619, 0.01763812, 0.01591992,
       0.01451437, 0.01334329, 0.01235258, 0.0115036 , 0.010768  ,
       0.01012451, 0.0095569 , 0.00905251, 0.00860136, 0.00819546,
       0.00782836, 0.00749477, 0.00719031, 0.00691134, 0.00665482,
       0.00641813, 0.0061991 , 0.00599582, 0.00580666, 0.00563022,
       0.00546526, 0.00531071, 0.00516563, 0.00502917, 0.0049006 ,
       0.00477927, 0.00466459, 0.00455603, 0.00445313, 0.00435545,
       0.00426262, 0.0041743 , 0.00409017, 0.00400994, 0.00393336,
       0.00386018, 0.0037902 , 0.00372321, 0.00365903, 0.00359749,
       0.00353844, 0.00348173, 0.00342724, 0.00337484, 0.00332441,
       0.00327586, 0.00322908, 0.00318398, 0.00314049, 0.00309851,
       0.00305798, 0.00301882, 0.00298097, 0.00294437, 0.00290897,
       0.00287471, 0.00284153, 0.0028094 , 0.00277826, 0.00274808,
       0.00271882, 0.00269043, 0.00266289, 0.00263615, 0.00261019,
       0.00258498, 0.00256049, 0.00253669, 0.00251356, 0.00249107,
       0.0024692 , 0.00244792, 0.00242722, 0.00240708, 0.00238747,
       0.00236839, 0.0023498 , 0.00233171, 0.00231408, 0.0022969 ,
       0.00228017, 0.00226387, 0.00224797, 0.00223248, 0.00221738,
       0.00220266, 0.00218831, 0.00217431, 0.00216065, 0.00214734,
       0.00213435, 0.00212168, 0.00210932, 0.00209726, 0.0020855 ,
       0.00207402, 0.00206283, 0.0020519 , 0.00204124, 0.00203084,
       0.00202069, 0.00201079, 0.00200113, 0.00199171, 0.00198251,
       0.00197354, 0.0019648 , 0.00195626, 0.00194794, 0.00193982,
       0.00193191, 0.00192419, 0.00191667, 0.00190934, 0.00190219,
       0.00189523, 0.00188845, 0.00188184, 0.0018754 , 0.00186914,
       0.00186304, 0.00185711, 0.00185134, 0.00184573, 0.00184027,
       0.00183497, 0.00182982, 0.00182481, 0.00181996, 0.00181525,
       0.00181068, 0.00180625, 0.00180196, 0.00179781, 0.00179379,
       0.00178991, 0.00178616, 0.00178254, 0.00177905, 0.00177568,
       0.00177245, 0.00176933, 0.00176634, 0.00176348, 0.00176073,
       0.0017581 , 0.0017556 , 0.00175321, 0.00175094, 0.00174878,
       0.00174674, 0.00174481, 0.001743  , 0.0017413 , 0.00173972,
       0.00173824, 0.00173688, 0.00173563, 0.00173449, 0.00173346,
       0.00173254, 0.00173172, 0.00173102, 0.00173043, 0.00172994,
       0.00172956, 0.00172929, 0.00172913, 0.00172908])
2025-11-10 11:37:04,189 - INFO - num_paired_subset_sizes = 178
2025-11-10 11:37:04,628 - INFO - weight_left = np.float64(0.9999999999999999)
2025-11-10 11:37:10,270 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:37:10,270 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:37:10,349 - INFO - phi = array([0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.00201319, 0.        , 0.00347236,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00299808, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00202149, 0.00230598, 0.        , 0.        , 0.        ,
       0.00204624, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.00212274, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.00214838,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00224238, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00202197, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        ])
2025-11-10 11:37:10,361 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:37:10,361 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:37:10,442 - INFO - phi = array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.00201319,  0.        , -0.00347236,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00299808,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00202149, -0.00230598,  0.        ,  0.        ,  0.        ,
       -0.00204624,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.00212274,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        , -0.00214838,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00224238,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00202197,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ])
2025-11-10 11:37:11,304 - INFO - num_full_subsets = 0
2025-11-10 11:37:11,306 - INFO - remaining_weight_vector = array([0.15491575, 0.07767303, 0.05192626, 0.03905348, 0.0313303 ,
       0.02618192, 0.02250486, 0.01974738, 0.01760295, 0.01588767,
       0.01448448, 0.01331538, 0.01232634, 0.01147877, 0.0107444 ,
       0.01010198, 0.00953531, 0.00903175, 0.00858134, 0.00817611,
       0.00780961, 0.00747655, 0.00717258, 0.00689406, 0.00663793,
       0.00640162, 0.00618293, 0.00597996, 0.00579109, 0.00561492,
       0.00545021, 0.00529589, 0.00515102, 0.00501476, 0.00488638,
       0.00476522, 0.00465069, 0.00454228, 0.00443952, 0.00434197,
       0.00424927, 0.00416106, 0.00407703, 0.0039969 , 0.00392041,
       0.00384732, 0.00377741, 0.0037105 , 0.00364638, 0.00358491,
       0.00352592, 0.00346927, 0.00341482, 0.00336247, 0.00331209,
       0.00326357, 0.00321683, 0.00317177, 0.0031283 , 0.00308635,
       0.00304584, 0.0030067 , 0.00296887, 0.00293229, 0.0028969 ,
       0.00286264, 0.00282948, 0.00279735, 0.00276622, 0.00273604,
       0.00270677, 0.00267838, 0.00265083, 0.00262409, 0.00259812,
       0.0025729 , 0.0025484 , 0.00252458, 0.00250144, 0.00247893,
       0.00245704, 0.00243574, 0.00241502, 0.00239485, 0.00237522,
       0.00235611, 0.0023375 , 0.00231937, 0.00230171, 0.0022845 ,
       0.00226773, 0.00225139, 0.00223546, 0.00221993, 0.00220479,
       0.00219003, 0.00217563, 0.00216159, 0.00214789, 0.00213453,
       0.00212149, 0.00210877, 0.00209636, 0.00208425, 0.00207243,
       0.0020609 , 0.00204965, 0.00203866, 0.00202794, 0.00201748,
       0.00200727, 0.00199731, 0.00198758, 0.00197809, 0.00196883,
       0.00195979, 0.00195097, 0.00194237, 0.00193397, 0.00192578,
       0.00191779, 0.00190999, 0.00190239, 0.00189498, 0.00188775,
       0.0018807 , 0.00187383, 0.00186714, 0.00186062, 0.00185426,
       0.00184807, 0.00184205, 0.00183618, 0.00183047, 0.00182492,
       0.00181951, 0.00181426, 0.00180915, 0.00180419, 0.00179938,
       0.0017947 , 0.00179016, 0.00178576, 0.00178149, 0.00177736,
       0.00177336, 0.00176949, 0.00176574, 0.00176213, 0.00175863,
       0.00175527, 0.00175202, 0.0017489 , 0.0017459 , 0.00174301,
       0.00174025, 0.0017376 , 0.00173506, 0.00173265, 0.00173034,
       0.00172815, 0.00172607, 0.0017241 , 0.00172224, 0.00172049,
       0.00171885, 0.00171732, 0.0017159 , 0.00171458, 0.00171338,
       0.00171227, 0.00171128, 0.00171039, 0.0017096 , 0.00170893,
       0.00170835, 0.00170788, 0.00170752, 0.00170726, 0.0017071 ,
       0.00170705])
2025-11-10 11:37:11,308 - INFO - num_paired_subset_sizes = 180
2025-11-10 11:37:11,755 - INFO - weight_left = np.float64(1.0)
2025-11-10 11:37:17,551 - INFO - np.sum(w_aug) = np.float64(362.0)
2025-11-10 11:37:17,552 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:37:17,643 - INFO - phi = array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.02331289,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        , -0.012122  ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.01743807,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.03016815,  0.        ,  0.        , -0.00802925,  0.        ,
        0.        ,  0.        ,  0.        , -0.01698172,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        , -0.01952352,
        0.        ,  0.        ,  0.        , -0.00560477,  0.        ,
        0.        ,  0.        ,  0.        , -0.021559  , -0.038526  ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ])
2025-11-10 11:37:17,654 - INFO - np.sum(w_aug) = np.float64(362.0)
2025-11-10 11:37:17,657 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:37:17,758 - INFO - phi = array([0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.02331289, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.012122  ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.01743807, 0.        , 0.        , 0.        , 0.        ,
       0.03016815, 0.        , 0.        , 0.00802925, 0.        ,
       0.        , 0.        , 0.        , 0.01698172, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.01952352,
       0.        , 0.        , 0.        , 0.00560477, 0.        ,
       0.        , 0.        , 0.        , 0.021559  , 0.038526  ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        ])
2025-11-10 11:37:18,628 - INFO - num_full_subsets = 0
2025-11-10 11:37:18,629 - INFO - remaining_weight_vector = array([0.15505029, 0.07774169, 0.05197297, 0.03908922, 0.03135947,
       0.02620671, 0.02252653, 0.01976671, 0.01762047, 0.01590373,
       0.01449936, 0.01332928, 0.0123394 , 0.01149113, 0.01075615,
       0.0101132 , 0.00954606, 0.00904208, 0.0085913 , 0.00818574,
       0.00781894, 0.00748562, 0.0071814 , 0.00690266, 0.00664633,
       0.00640984, 0.00619097, 0.00598785, 0.00579884, 0.00562253,
       0.0054577 , 0.00530326, 0.00515828, 0.00502193, 0.00489346,
       0.00477221, 0.00465761, 0.00454912, 0.00444629, 0.00434868,
       0.00425591, 0.00416764, 0.00408356, 0.00400338, 0.00392685,
       0.00385371, 0.00378377, 0.00371682, 0.00365267, 0.00359116,
       0.00353214, 0.00347547, 0.003421  , 0.00336862, 0.00331821,
       0.00326968, 0.00322292, 0.00317784, 0.00313436, 0.00309239,
       0.00305187, 0.00301272, 0.00297488, 0.00293829, 0.0029029 ,
       0.00286864, 0.00283547, 0.00280334, 0.0027722 , 0.00274202,
       0.00271276, 0.00268437, 0.00265682, 0.00263008, 0.00260412,
       0.0025789 , 0.00255441, 0.0025306 , 0.00250746, 0.00248496,
       0.00246308, 0.00244179, 0.00242108, 0.00240093, 0.00238131,
       0.00236221, 0.00234361, 0.0023255 , 0.00230785, 0.00229066,
       0.00227391, 0.00225759, 0.00224168, 0.00222617, 0.00221104,
       0.0021963 , 0.00218192, 0.0021679 , 0.00215423, 0.00214089,
       0.00212787, 0.00211518, 0.00210279, 0.00209071, 0.00207892,
       0.00206741, 0.00205619, 0.00204523, 0.00203454, 0.00202411,
       0.00201393, 0.002004  , 0.00199431, 0.00198485, 0.00197562,
       0.00196661, 0.00195783, 0.00194926, 0.0019409 , 0.00193274,
       0.00192479, 0.00191704, 0.00190947, 0.0019021 , 0.00189491,
       0.00188791, 0.00188108, 0.00187443, 0.00186795, 0.00186164,
       0.0018555 , 0.00184952, 0.0018437 , 0.00183804, 0.00183253,
       0.00182717, 0.00182197, 0.00181692, 0.00181201, 0.00180724,
       0.00180262, 0.00179813, 0.00179379, 0.00178958, 0.0017855 ,
       0.00178156, 0.00177775, 0.00177406, 0.00177051, 0.00176708,
       0.00176378, 0.0017606 , 0.00175754, 0.0017546 , 0.00175179,
       0.00174909, 0.00174651, 0.00174405, 0.0017417 , 0.00173947,
       0.00173735, 0.00173535, 0.00173346, 0.00173168, 0.00173001,
       0.00172845, 0.001727  , 0.00172567, 0.00172444, 0.00172331,
       0.0017223 , 0.0017214 , 0.0017206 , 0.00171991, 0.00171932,
       0.00171884, 0.00171847, 0.00171821, 0.00171805, 0.001718  ])
2025-11-10 11:37:18,631 - INFO - num_paired_subset_sizes = 179
2025-11-10 11:37:19,065 - INFO - weight_left = np.float64(1.0)
2025-11-10 11:37:24,603 - INFO - np.sum(w_aug) = np.float64(360.0)
2025-11-10 11:37:24,603 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:37:24,700 - INFO - phi = array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.00143807,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.00199474,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.00191887,  0.00158339,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.0020645 ,  0.00473311, -0.00570233,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.00370163,  0.00412753, -0.00426951,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])
2025-11-10 11:37:24,709 - INFO - np.sum(w_aug) = np.float64(360.0)
2025-11-10 11:37:24,709 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:37:24,779 - INFO - phi = array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00143807,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00199474,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.00191887, -0.00158339,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.0020645 , -0.00473311,  0.00570233,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.00370163, -0.00412753,  0.00426951,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])
2025-11-10 11:37:25,655 - INFO - num_full_subsets = 0
2025-11-10 11:37:25,657 - INFO - remaining_weight_vector = array([0.15518582, 0.07781087, 0.05202003, 0.03912524, 0.03138886,
       0.02623169, 0.02254837, 0.01978619, 0.01763812, 0.01591992,
       0.01451437, 0.01334329, 0.01235258, 0.0115036 , 0.010768  ,
       0.01012451, 0.0095569 , 0.00905251, 0.00860136, 0.00819546,
       0.00782836, 0.00749477, 0.00719031, 0.00691134, 0.00665482,
       0.00641813, 0.0061991 , 0.00599582, 0.00580666, 0.00563022,
       0.00546526, 0.00531071, 0.00516563, 0.00502917, 0.0049006 ,
       0.00477927, 0.00466459, 0.00455603, 0.00445313, 0.00435545,
       0.00426262, 0.0041743 , 0.00409017, 0.00400994, 0.00393336,
       0.00386018, 0.0037902 , 0.00372321, 0.00365903, 0.00359749,
       0.00353844, 0.00348173, 0.00342724, 0.00337484, 0.00332441,
       0.00327586, 0.00322908, 0.00318398, 0.00314049, 0.00309851,
       0.00305798, 0.00301882, 0.00298097, 0.00294437, 0.00290897,
       0.00287471, 0.00284153, 0.0028094 , 0.00277826, 0.00274808,
       0.00271882, 0.00269043, 0.00266289, 0.00263615, 0.00261019,
       0.00258498, 0.00256049, 0.00253669, 0.00251356, 0.00249107,
       0.0024692 , 0.00244792, 0.00242722, 0.00240708, 0.00238747,
       0.00236839, 0.0023498 , 0.00233171, 0.00231408, 0.0022969 ,
       0.00228017, 0.00226387, 0.00224797, 0.00223248, 0.00221738,
       0.00220266, 0.00218831, 0.00217431, 0.00216065, 0.00214734,
       0.00213435, 0.00212168, 0.00210932, 0.00209726, 0.0020855 ,
       0.00207402, 0.00206283, 0.0020519 , 0.00204124, 0.00203084,
       0.00202069, 0.00201079, 0.00200113, 0.00199171, 0.00198251,
       0.00197354, 0.0019648 , 0.00195626, 0.00194794, 0.00193982,
       0.00193191, 0.00192419, 0.00191667, 0.00190934, 0.00190219,
       0.00189523, 0.00188845, 0.00188184, 0.0018754 , 0.00186914,
       0.00186304, 0.00185711, 0.00185134, 0.00184573, 0.00184027,
       0.00183497, 0.00182982, 0.00182481, 0.00181996, 0.00181525,
       0.00181068, 0.00180625, 0.00180196, 0.00179781, 0.00179379,
       0.00178991, 0.00178616, 0.00178254, 0.00177905, 0.00177568,
       0.00177245, 0.00176933, 0.00176634, 0.00176348, 0.00176073,
       0.0017581 , 0.0017556 , 0.00175321, 0.00175094, 0.00174878,
       0.00174674, 0.00174481, 0.001743  , 0.0017413 , 0.00173972,
       0.00173824, 0.00173688, 0.00173563, 0.00173449, 0.00173346,
       0.00173254, 0.00173172, 0.00173102, 0.00173043, 0.00172994,
       0.00172956, 0.00172929, 0.00172913, 0.00172908])
2025-11-10 11:37:25,659 - INFO - num_paired_subset_sizes = 178
2025-11-10 11:37:26,137 - INFO - weight_left = np.float64(0.9999999999999999)
2025-11-10 11:37:31,548 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:37:31,549 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:37:31,628 - INFO - phi = array([0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.00161313, 0.00287795, 0.00286606,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00249454, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00230419, 0.00225823, 0.        , 0.        , 0.        ,
       0.00211945, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00218397, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00201721, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.00220611, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        ])
2025-11-10 11:37:31,636 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:37:31,637 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:37:31,718 - INFO - phi = array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.00161313, -0.00287795, -0.00286606,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00249454,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00230419, -0.00225823,  0.        ,  0.        ,  0.        ,
       -0.00211945,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00218397,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00201721,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.00220611,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ])
2025-11-10 11:37:32,551 - INFO - num_full_subsets = 0
2025-11-10 11:37:32,553 - INFO - remaining_weight_vector = array([0.15518582, 0.07781087, 0.05202003, 0.03912524, 0.03138886,
       0.02623169, 0.02254837, 0.01978619, 0.01763812, 0.01591992,
       0.01451437, 0.01334329, 0.01235258, 0.0115036 , 0.010768  ,
       0.01012451, 0.0095569 , 0.00905251, 0.00860136, 0.00819546,
       0.00782836, 0.00749477, 0.00719031, 0.00691134, 0.00665482,
       0.00641813, 0.0061991 , 0.00599582, 0.00580666, 0.00563022,
       0.00546526, 0.00531071, 0.00516563, 0.00502917, 0.0049006 ,
       0.00477927, 0.00466459, 0.00455603, 0.00445313, 0.00435545,
       0.00426262, 0.0041743 , 0.00409017, 0.00400994, 0.00393336,
       0.00386018, 0.0037902 , 0.00372321, 0.00365903, 0.00359749,
       0.00353844, 0.00348173, 0.00342724, 0.00337484, 0.00332441,
       0.00327586, 0.00322908, 0.00318398, 0.00314049, 0.00309851,
       0.00305798, 0.00301882, 0.00298097, 0.00294437, 0.00290897,
       0.00287471, 0.00284153, 0.0028094 , 0.00277826, 0.00274808,
       0.00271882, 0.00269043, 0.00266289, 0.00263615, 0.00261019,
       0.00258498, 0.00256049, 0.00253669, 0.00251356, 0.00249107,
       0.0024692 , 0.00244792, 0.00242722, 0.00240708, 0.00238747,
       0.00236839, 0.0023498 , 0.00233171, 0.00231408, 0.0022969 ,
       0.00228017, 0.00226387, 0.00224797, 0.00223248, 0.00221738,
       0.00220266, 0.00218831, 0.00217431, 0.00216065, 0.00214734,
       0.00213435, 0.00212168, 0.00210932, 0.00209726, 0.0020855 ,
       0.00207402, 0.00206283, 0.0020519 , 0.00204124, 0.00203084,
       0.00202069, 0.00201079, 0.00200113, 0.00199171, 0.00198251,
       0.00197354, 0.0019648 , 0.00195626, 0.00194794, 0.00193982,
       0.00193191, 0.00192419, 0.00191667, 0.00190934, 0.00190219,
       0.00189523, 0.00188845, 0.00188184, 0.0018754 , 0.00186914,
       0.00186304, 0.00185711, 0.00185134, 0.00184573, 0.00184027,
       0.00183497, 0.00182982, 0.00182481, 0.00181996, 0.00181525,
       0.00181068, 0.00180625, 0.00180196, 0.00179781, 0.00179379,
       0.00178991, 0.00178616, 0.00178254, 0.00177905, 0.00177568,
       0.00177245, 0.00176933, 0.00176634, 0.00176348, 0.00176073,
       0.0017581 , 0.0017556 , 0.00175321, 0.00175094, 0.00174878,
       0.00174674, 0.00174481, 0.001743  , 0.0017413 , 0.00173972,
       0.00173824, 0.00173688, 0.00173563, 0.00173449, 0.00173346,
       0.00173254, 0.00173172, 0.00173102, 0.00173043, 0.00172994,
       0.00172956, 0.00172929, 0.00172913, 0.00172908])
2025-11-10 11:37:32,555 - INFO - num_paired_subset_sizes = 178
2025-11-10 11:37:32,998 - INFO - weight_left = np.float64(0.9999999999999999)
2025-11-10 11:37:38,368 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:37:38,369 - INFO - np.sum(self.kernelWeights) = np.float64(0.9999999999999999)
2025-11-10 11:37:38,448 - INFO - phi = array([0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00187287, 0.00305035,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00274457, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00218517, 0.        , 0.        , 0.        , 0.        ,
       0.00193687, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.00182612, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.00235734,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00200086, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00224898, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.00218512, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        ])
2025-11-10 11:37:38,456 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:37:38,457 - INFO - np.sum(self.kernelWeights) = np.float64(0.9999999999999999)
2025-11-10 11:37:38,537 - INFO - phi = array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00187287, -0.00305035,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00274457,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00218517,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00193687,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.00182612,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        , -0.00235734,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00200086,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00224898,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.00218512,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ])
2025-11-10 11:37:39,425 - INFO - num_full_subsets = 0
2025-11-10 11:37:39,426 - INFO - remaining_weight_vector = array([0.15412863, 0.07727148, 0.05165317, 0.03884458, 0.03115988,
       0.02603713, 0.02237835, 0.01963456, 0.01750075, 0.01579395,
       0.01439769, 0.01323434, 0.01225016, 0.01140674, 0.01067595,
       0.01003666, 0.00947273, 0.00897159, 0.00852335, 0.00812005,
       0.00775529, 0.00742381, 0.00712127, 0.00684405, 0.00658911,
       0.00635389, 0.00613619, 0.00593414, 0.00574612, 0.00557073,
       0.00540675, 0.0052531 , 0.00510886, 0.00497318, 0.00484534,
       0.00472469, 0.00461063, 0.00450266, 0.0044003 , 0.00430314,
       0.00421079, 0.00412292, 0.0040392 , 0.00395936, 0.00388315,
       0.00381031, 0.00374065, 0.00367395, 0.00361005, 0.00354876,
       0.00348995, 0.00343347, 0.00337918, 0.00332697, 0.00327672,
       0.00322832, 0.00318169, 0.00313673, 0.00309335, 0.00305149,
       0.00301105, 0.00297198, 0.00293421, 0.00289768, 0.00286233,
       0.00282812, 0.00279498, 0.00276288, 0.00273176, 0.0027016 ,
       0.00267234, 0.00264395, 0.00261639, 0.00258964, 0.00256366,
       0.00253841, 0.00251388, 0.00249004, 0.00246685, 0.0024443 ,
       0.00242236, 0.00240102, 0.00238024, 0.00236002, 0.00234032,
       0.00232114, 0.00230245, 0.00228425, 0.00226651, 0.00224922,
       0.00223236, 0.00221593, 0.0021999 , 0.00218427, 0.00216902,
       0.00215415, 0.00213964, 0.00212548, 0.00211166, 0.00209817,
       0.00208501, 0.00207216, 0.00205961, 0.00204736, 0.0020354 ,
       0.00202372, 0.00201232, 0.00200118, 0.00199031, 0.00197968,
       0.00196931, 0.00195917, 0.00194928, 0.00193961, 0.00193017,
       0.00192094, 0.00191194, 0.00190314, 0.00189455, 0.00188615,
       0.00187796, 0.00186996, 0.00186214, 0.00185452, 0.00184707,
       0.0018398 , 0.0018327 , 0.00182577, 0.00181902, 0.00181242,
       0.00180599, 0.00179971, 0.00179359, 0.00178762, 0.00178181,
       0.00177614, 0.00177061, 0.00176523, 0.00175999, 0.00175488,
       0.00174992, 0.00174508, 0.00174038, 0.00173581, 0.00173137,
       0.00172705, 0.00172286, 0.00171879, 0.00171484, 0.00171101,
       0.0017073 , 0.00170371, 0.00170023, 0.00169687, 0.00169362,
       0.00169048, 0.00168746, 0.00168454, 0.00168173, 0.00167903,
       0.00167643, 0.00167395, 0.00167156, 0.00166928, 0.0016671 ,
       0.00166502, 0.00166305, 0.00166118, 0.0016594 , 0.00165773,
       0.00165615, 0.00165467, 0.00165329, 0.00165201, 0.00165082,
       0.00164974, 0.00164874, 0.00164784, 0.00164704, 0.00164633,
       0.00164572, 0.0016452 , 0.00164478, 0.00164445, 0.00164422,
       0.00164407, 0.00164403])
2025-11-10 11:37:39,428 - INFO - num_paired_subset_sizes = 186
2025-11-10 11:37:39,897 - INFO - weight_left = np.float64(1.0)
2025-11-10 11:37:45,016 - INFO - np.sum(w_aug) = np.float64(374.00000000000006)
2025-11-10 11:37:45,016 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:37:45,096 - INFO - phi = array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        , -0.02486522,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.02136925,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.01021124,  0.        , -0.00874215,  0.        ,  0.        ,
        0.        ,  0.        , -0.01336906,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        , -0.04217224,
       -0.02623465,  0.        ,  0.        ,  0.        , -0.01513401,
        0.        , -0.01485   ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00987202,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ])
2025-11-10 11:37:45,105 - INFO - np.sum(w_aug) = np.float64(374.00000000000006)
2025-11-10 11:37:45,106 - INFO - np.sum(self.kernelWeights) = np.float64(1.0)
2025-11-10 11:37:45,191 - INFO - phi = array([0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.02486522,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.02136925, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.01021124, 0.        , 0.00874215, 0.        , 0.        ,
       0.        , 0.        , 0.01336906, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.04217224,
       0.02623465, 0.        , 0.        , 0.        , 0.01513401,
       0.        , 0.01485   , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00987202, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        ])
2025-11-10 11:37:46,019 - INFO - num_full_subsets = 0
2025-11-10 11:37:46,021 - INFO - remaining_weight_vector = array([0.15518582, 0.07781087, 0.05202003, 0.03912524, 0.03138886,
       0.02623169, 0.02254837, 0.01978619, 0.01763812, 0.01591992,
       0.01451437, 0.01334329, 0.01235258, 0.0115036 , 0.010768  ,
       0.01012451, 0.0095569 , 0.00905251, 0.00860136, 0.00819546,
       0.00782836, 0.00749477, 0.00719031, 0.00691134, 0.00665482,
       0.00641813, 0.0061991 , 0.00599582, 0.00580666, 0.00563022,
       0.00546526, 0.00531071, 0.00516563, 0.00502917, 0.0049006 ,
       0.00477927, 0.00466459, 0.00455603, 0.00445313, 0.00435545,
       0.00426262, 0.0041743 , 0.00409017, 0.00400994, 0.00393336,
       0.00386018, 0.0037902 , 0.00372321, 0.00365903, 0.00359749,
       0.00353844, 0.00348173, 0.00342724, 0.00337484, 0.00332441,
       0.00327586, 0.00322908, 0.00318398, 0.00314049, 0.00309851,
       0.00305798, 0.00301882, 0.00298097, 0.00294437, 0.00290897,
       0.00287471, 0.00284153, 0.0028094 , 0.00277826, 0.00274808,
       0.00271882, 0.00269043, 0.00266289, 0.00263615, 0.00261019,
       0.00258498, 0.00256049, 0.00253669, 0.00251356, 0.00249107,
       0.0024692 , 0.00244792, 0.00242722, 0.00240708, 0.00238747,
       0.00236839, 0.0023498 , 0.00233171, 0.00231408, 0.0022969 ,
       0.00228017, 0.00226387, 0.00224797, 0.00223248, 0.00221738,
       0.00220266, 0.00218831, 0.00217431, 0.00216065, 0.00214734,
       0.00213435, 0.00212168, 0.00210932, 0.00209726, 0.0020855 ,
       0.00207402, 0.00206283, 0.0020519 , 0.00204124, 0.00203084,
       0.00202069, 0.00201079, 0.00200113, 0.00199171, 0.00198251,
       0.00197354, 0.0019648 , 0.00195626, 0.00194794, 0.00193982,
       0.00193191, 0.00192419, 0.00191667, 0.00190934, 0.00190219,
       0.00189523, 0.00188845, 0.00188184, 0.0018754 , 0.00186914,
       0.00186304, 0.00185711, 0.00185134, 0.00184573, 0.00184027,
       0.00183497, 0.00182982, 0.00182481, 0.00181996, 0.00181525,
       0.00181068, 0.00180625, 0.00180196, 0.00179781, 0.00179379,
       0.00178991, 0.00178616, 0.00178254, 0.00177905, 0.00177568,
       0.00177245, 0.00176933, 0.00176634, 0.00176348, 0.00176073,
       0.0017581 , 0.0017556 , 0.00175321, 0.00175094, 0.00174878,
       0.00174674, 0.00174481, 0.001743  , 0.0017413 , 0.00173972,
       0.00173824, 0.00173688, 0.00173563, 0.00173449, 0.00173346,
       0.00173254, 0.00173172, 0.00173102, 0.00173043, 0.00172994,
       0.00172956, 0.00172929, 0.00172913, 0.00172908])
2025-11-10 11:37:46,023 - INFO - num_paired_subset_sizes = 178
2025-11-10 11:37:46,455 - INFO - weight_left = np.float64(0.9999999999999999)
2025-11-10 11:37:52,008 - INFO - np.sum(w_aug) = np.float64(357.99999999999994)
2025-11-10 11:37:52,009 - INFO - np.sum(self.kernelWeights) = np.float64(0.9999999999999997)
2025-11-10 11:37:52,089 - INFO - phi = array([0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00231122, 0.00307448,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00238668, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00245534, 0.002378  , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.00223656, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.00232514,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.00227225, 0.        , 0.00277415,
       0.        , 0.        , 0.00250175, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        ])
2025-11-10 11:37:52,098 - INFO - np.sum(w_aug) = np.float64(357.99999999999994)
2025-11-10 11:37:52,099 - INFO - np.sum(self.kernelWeights) = np.float64(0.9999999999999997)
2025-11-10 11:37:52,173 - INFO - phi = array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00231122, -0.00307448,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00238668,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00245534, -0.002378  ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.00223656,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        , -0.00232514,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.00227225,  0.        , -0.00277415,
        0.        ,  0.        , -0.00250175,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ])
2025-11-10 11:37:53,047 - INFO - num_full_subsets = 0
2025-11-10 11:37:53,048 - INFO - remaining_weight_vector = array([0.15518582, 0.07781087, 0.05202003, 0.03912524, 0.03138886,
       0.02623169, 0.02254837, 0.01978619, 0.01763812, 0.01591992,
       0.01451437, 0.01334329, 0.01235258, 0.0115036 , 0.010768  ,
       0.01012451, 0.0095569 , 0.00905251, 0.00860136, 0.00819546,
       0.00782836, 0.00749477, 0.00719031, 0.00691134, 0.00665482,
       0.00641813, 0.0061991 , 0.00599582, 0.00580666, 0.00563022,
       0.00546526, 0.00531071, 0.00516563, 0.00502917, 0.0049006 ,
       0.00477927, 0.00466459, 0.00455603, 0.00445313, 0.00435545,
       0.00426262, 0.0041743 , 0.00409017, 0.00400994, 0.00393336,
       0.00386018, 0.0037902 , 0.00372321, 0.00365903, 0.00359749,
       0.00353844, 0.00348173, 0.00342724, 0.00337484, 0.00332441,
       0.00327586, 0.00322908, 0.00318398, 0.00314049, 0.00309851,
       0.00305798, 0.00301882, 0.00298097, 0.00294437, 0.00290897,
       0.00287471, 0.00284153, 0.0028094 , 0.00277826, 0.00274808,
       0.00271882, 0.00269043, 0.00266289, 0.00263615, 0.00261019,
       0.00258498, 0.00256049, 0.00253669, 0.00251356, 0.00249107,
       0.0024692 , 0.00244792, 0.00242722, 0.00240708, 0.00238747,
       0.00236839, 0.0023498 , 0.00233171, 0.00231408, 0.0022969 ,
       0.00228017, 0.00226387, 0.00224797, 0.00223248, 0.00221738,
       0.00220266, 0.00218831, 0.00217431, 0.00216065, 0.00214734,
       0.00213435, 0.00212168, 0.00210932, 0.00209726, 0.0020855 ,
       0.00207402, 0.00206283, 0.0020519 , 0.00204124, 0.00203084,
       0.00202069, 0.00201079, 0.00200113, 0.00199171, 0.00198251,
       0.00197354, 0.0019648 , 0.00195626, 0.00194794, 0.00193982,
       0.00193191, 0.00192419, 0.00191667, 0.00190934, 0.00190219,
       0.00189523, 0.00188845, 0.00188184, 0.0018754 , 0.00186914,
       0.00186304, 0.00185711, 0.00185134, 0.00184573, 0.00184027,
       0.00183497, 0.00182982, 0.00182481, 0.00181996, 0.00181525,
       0.00181068, 0.00180625, 0.00180196, 0.00179781, 0.00179379,
       0.00178991, 0.00178616, 0.00178254, 0.00177905, 0.00177568,
       0.00177245, 0.00176933, 0.00176634, 0.00176348, 0.00176073,
       0.0017581 , 0.0017556 , 0.00175321, 0.00175094, 0.00174878,
       0.00174674, 0.00174481, 0.001743  , 0.0017413 , 0.00173972,
       0.00173824, 0.00173688, 0.00173563, 0.00173449, 0.00173346,
       0.00173254, 0.00173172, 0.00173102, 0.00173043, 0.00172994,
       0.00172956, 0.00172929, 0.00172913, 0.00172908])
2025-11-10 11:37:53,050 - INFO - num_paired_subset_sizes = 178
2025-11-10 11:37:53,478 - INFO - weight_left = np.float64(0.9999999999999999)
2025-11-10 11:37:59,203 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:37:59,204 - INFO - np.sum(self.kernelWeights) = np.float64(0.9999999999999998)
2025-11-10 11:37:59,283 - INFO - phi = array([0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.00180769, 0.00162866, 0.00260987,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00235917, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00196717, 0.00211662, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.00210633,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00214318, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.00217088, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00198718, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        ])
2025-11-10 11:37:59,291 - INFO - np.sum(w_aug) = np.float64(358.0)
2025-11-10 11:37:59,291 - INFO - np.sum(self.kernelWeights) = np.float64(0.9999999999999998)
2025-11-10 11:37:59,362 - INFO - phi = array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.00180769, -0.00162866, -0.00260987,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00235917,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00196717, -0.00211662,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        , -0.00210633,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.00214318,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        , -0.00217088,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.00198718,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ])
2025-11-10 11:38:00,193 - INFO - num_full_subsets = 0
2025-11-10 11:38:00,195 - INFO - remaining_weight_vector = array([0.15518582, 0.07781087, 0.05202003, 0.03912524, 0.03138886,
       0.02623169, 0.02254837, 0.01978619, 0.01763812, 0.01591992,
       0.01451437, 0.01334329, 0.01235258, 0.0115036 , 0.010768  ,
       0.01012451, 0.0095569 , 0.00905251, 0.00860136, 0.00819546,
       0.00782836, 0.00749477, 0.00719031, 0.00691134, 0.00665482,
       0.00641813, 0.0061991 , 0.00599582, 0.00580666, 0.00563022,
       0.00546526, 0.00531071, 0.00516563, 0.00502917, 0.0049006 ,
       0.00477927, 0.00466459, 0.00455603, 0.00445313, 0.00435545,
       0.00426262, 0.0041743 , 0.00409017, 0.00400994, 0.00393336,
       0.00386018, 0.0037902 , 0.00372321, 0.00365903, 0.00359749,
       0.00353844, 0.00348173, 0.00342724, 0.00337484, 0.00332441,
       0.00327586, 0.00322908, 0.00318398, 0.00314049, 0.00309851,
       0.00305798, 0.00301882, 0.00298097, 0.00294437, 0.00290897,
       0.00287471, 0.00284153, 0.0028094 , 0.00277826, 0.00274808,
       0.00271882, 0.00269043, 0.00266289, 0.00263615, 0.00261019,
       0.00258498, 0.00256049, 0.00253669, 0.00251356, 0.00249107,
       0.0024692 , 0.00244792, 0.00242722, 0.00240708, 0.00238747,
       0.00236839, 0.0023498 , 0.00233171, 0.00231408, 0.0022969 ,
       0.00228017, 0.00226387, 0.00224797, 0.00223248, 0.00221738,
       0.00220266, 0.00218831, 0.00217431, 0.00216065, 0.00214734,
       0.00213435, 0.00212168, 0.00210932, 0.00209726, 0.0020855 ,
       0.00207402, 0.00206283, 0.0020519 , 0.00204124, 0.00203084,
       0.00202069, 0.00201079, 0.00200113, 0.00199171, 0.00198251,
       0.00197354, 0.0019648 , 0.00195626, 0.00194794, 0.00193982,
       0.00193191, 0.00192419, 0.00191667, 0.00190934, 0.00190219,
       0.00189523, 0.00188845, 0.00188184, 0.0018754 , 0.00186914,
       0.00186304, 0.00185711, 0.00185134, 0.00184573, 0.00184027,
       0.00183497, 0.00182982, 0.00182481, 0.00181996, 0.00181525,
       0.00181068, 0.00180625, 0.00180196, 0.00179781, 0.00179379,
       0.00178991, 0.00178616, 0.00178254, 0.00177905, 0.00177568,
       0.00177245, 0.00176933, 0.00176634, 0.00176348, 0.00176073,
       0.0017581 , 0.0017556 , 0.00175321, 0.00175094, 0.00174878,
       0.00174674, 0.00174481, 0.001743  , 0.0017413 , 0.00173972,
       0.00173824, 0.00173688, 0.00173563, 0.00173449, 0.00173346,
       0.00173254, 0.00173172, 0.00173102, 0.00173043, 0.00172994,
       0.00172956, 0.00172929, 0.00172913, 0.00172908])
2025-11-10 11:38:00,197 - INFO - num_paired_subset_sizes = 178
2025-11-10 11:38:00,655 - INFO - weight_left = np.float64(0.9999999999999999)
2025-11-10 11:40:19,292 - INFO - Strating benchmarking...
2025-11-10 11:40:19,292 - INFO - Random seed beállítva: 42
2025-11-10 11:40:19,293 - INFO - Load and preprocess data...
2025-11-10 11:40:19,294 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-10 11:40:19,729 - INFO - Nyers adat sikeresen betöltve innen: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-10 11:40:19,817 - INFO - Adatok szétválasztva. Train méret: (8000, 393), Test méret: (2000, 393)
2025-11-10 11:40:19,843 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-10 11:40:19,847 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-10 11:40:19,848 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-10 11:40:20,538 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-10 11:40:20,600 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-10 11:40:29,363 - INFO - Feldolgozott train/test szettek elmentve a(z) 'E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-10 11:40:29,380 - INFO - Successfuly loaded data. Train size: (8000, 505), Test size: (2000, 505)
2025-11-10 11:40:29,381 - INFO - Starting iteration with following models: ['logistic_regression', 'random_forest', 'gradient_boosting']
2025-11-10 11:40:29,381 - INFO - --- Model: logistic_regression ---
2025-11-10 11:40:29,382 - INFO - Initialize model parameters from config...
2025-11-10 11:40:29,382 - INFO - Training and evaluating...
2025-11-10 11:40:29,382 - INFO - Starting model training: LogisticRegression...
2025-11-10 11:40:36,349 - INFO - LogisticRegression training finished.
2025-11-10 11:40:36,349 - INFO - Prediction on test dataset
2025-11-10 11:40:36,360 - INFO - Metrikák számítása...
2025-11-10 11:40:36,365 - INFO - Results (logistic_regression): {'accuracy': 0.9645, 'f1_score': 0.9599949731165981, 'pr_auc': 0.27440597908410147}
2025-11-10 11:40:36,367 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\logistic_regression.pkl
2025-11-10 11:40:36,367 - INFO - SHAP magyarázat generálása (logistic_regression)...
2025-11-10 11:40:36,368 - INFO - Starting SHAP explanation on model: logistic_regression...
2025-11-10 11:40:36,369 - INFO - Calculating SHAP values...
2025-11-10 11:40:36,452 - INFO - SHAP calculation finished.
2025-11-10 11:40:36,452 - INFO - Generating SHAP beeswarm plot...
2025-11-10 11:40:36,733 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_beeswarm.png
2025-11-10 11:40:36,734 - INFO - Generating SHAP bar plot...
2025-11-10 11:40:36,979 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_bar.png
2025-11-10 11:40:36,982 - INFO - SHAP magyarázat elmentve.
2025-11-10 11:40:36,982 - INFO - --- Model: random_forest ---
2025-11-10 11:40:36,983 - INFO - Initialize model parameters from config...
2025-11-10 11:40:36,983 - INFO - Training and evaluating...
2025-11-10 11:40:36,983 - INFO - Starting model training: RandomForestClassifier...
2025-11-10 11:40:39,508 - INFO - RandomForestClassifier training finished.
2025-11-10 11:40:39,509 - INFO - Prediction on test dataset
2025-11-10 11:40:39,574 - INFO - Metrikák számítása...
2025-11-10 11:40:39,581 - INFO - Results (random_forest): {'accuracy': 0.9705, 'f1_score': 0.9617141505914562, 'pr_auc': 0.3807479382619139}
2025-11-10 11:40:39,588 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\random_forest.pkl
2025-11-10 11:40:39,588 - INFO - SHAP magyarázat generálása (random_forest)...
2025-11-10 11:40:39,589 - INFO - Starting SHAP explanation on model: random_forest...
2025-11-10 11:40:39,599 - INFO - Calculating SHAP values...
2025-11-10 11:40:53,189 - ERROR - Hiba a SHAP magyarázat generálása során (random_forest): Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.254216, while the model output was 0.234216. If this difference is acceptable you can set check_additivity=False to disable this check.
Traceback (most recent call last):
  File "e:\Users\G9179\source\xai_fraud_detection_benchmark\src\explain.py", line 67, in generate_shap_values
    shap_values_obj = explainer(X_to_explain)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 380, in __call__
    v = self.shap_values(X, y=y, from_call=True, check_additivity=check_additivity, approximate=approximate)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 677, in shap_values
    self.assert_additivity(out, self.model.predict(X))
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 862, in assert_additivity
    check_sum(self.expected_value[i] + phi[i].sum(-1), model_output[:, i])
    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 858, in check_sum
    raise ExplainerError(err_msg)
shap.utils._exceptions.ExplainerError: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.254216, while the model output was 0.234216. If this difference is acceptable you can set check_additivity=False to disable this check.
2025-11-10 11:40:53,197 - INFO - SHAP magyarázat elmentve.
2025-11-10 11:40:53,197 - INFO - --- Model: gradient_boosting ---
2025-11-10 11:40:53,198 - INFO - Initialize model parameters from config...
2025-11-10 11:40:53,198 - INFO - Training and evaluating...
2025-11-10 11:40:53,198 - INFO - Starting model training: GradientBoostingClassifier...
2025-11-10 11:41:23,738 - INFO - Strating benchmarking...
2025-11-10 11:41:23,739 - INFO - Random seed beállítva: 42
2025-11-10 11:41:23,740 - INFO - Load and preprocess data...
2025-11-10 11:41:23,741 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-10 11:41:24,192 - INFO - Nyers adat sikeresen betöltve innen: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-10 11:41:24,262 - INFO - Adatok szétválasztva. Train méret: (8000, 393), Test méret: (2000, 393)
2025-11-10 11:41:24,286 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-10 11:41:24,290 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-10 11:41:24,290 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-10 11:41:25,000 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-10 11:41:25,072 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-10 11:41:33,002 - INFO - Feldolgozott train/test szettek elmentve a(z) 'E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-10 11:41:33,017 - INFO - Successfuly loaded data. Train size: (8000, 505), Test size: (2000, 505)
2025-11-10 11:41:33,018 - INFO - Starting iteration with following models: ['logistic_regression', 'random_forest', 'gradient_boosting']
2025-11-10 11:41:33,018 - INFO - --- Model: logistic_regression ---
2025-11-10 11:41:33,019 - INFO - Initialize model parameters from config...
2025-11-10 11:41:33,019 - INFO - Training and evaluating...
2025-11-10 11:41:33,019 - INFO - Starting model training: LogisticRegression...
2025-11-10 11:41:40,310 - INFO - LogisticRegression training finished.
2025-11-10 11:41:40,311 - INFO - Prediction on test dataset
2025-11-10 11:41:40,324 - INFO - Metrikák számítása...
2025-11-10 11:41:40,331 - INFO - Results (logistic_regression): {'accuracy': 0.9645, 'f1_score': 0.9599949731165981, 'pr_auc': 0.27440597908410147}
2025-11-10 11:41:40,332 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\logistic_regression.pkl
2025-11-10 11:41:40,333 - INFO - SHAP magyarázat generálása (logistic_regression)...
2025-11-10 11:41:40,333 - INFO - Starting SHAP explanation on model: logistic_regression...
2025-11-10 11:41:40,335 - INFO - Calculating SHAP values...
2025-11-10 11:41:40,426 - INFO - SHAP calculation finished.
2025-11-10 11:41:40,426 - INFO - Generating SHAP beeswarm plot...
2025-11-10 11:41:40,739 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_beeswarm.png
2025-11-10 11:41:40,740 - INFO - Generating SHAP bar plot...
2025-11-10 11:41:40,987 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_bar.png
2025-11-10 11:41:40,990 - INFO - SHAP magyarázat elmentve.
2025-11-10 11:41:40,990 - INFO - --- Model: random_forest ---
2025-11-10 11:41:40,991 - INFO - Initialize model parameters from config...
2025-11-10 11:41:40,991 - INFO - Training and evaluating...
2025-11-10 11:41:40,991 - INFO - Starting model training: RandomForestClassifier...
2025-11-10 11:41:43,409 - INFO - RandomForestClassifier training finished.
2025-11-10 11:41:43,409 - INFO - Prediction on test dataset
2025-11-10 11:41:43,477 - INFO - Metrikák számítása...
2025-11-10 11:41:43,484 - INFO - Results (random_forest): {'accuracy': 0.9705, 'f1_score': 0.9617141505914562, 'pr_auc': 0.3807479382619139}
2025-11-10 11:41:43,491 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\random_forest.pkl
2025-11-10 11:41:43,491 - INFO - SHAP magyarázat generálása (random_forest)...
2025-11-10 11:41:43,491 - INFO - Starting SHAP explanation on model: random_forest...
2025-11-10 11:41:43,510 - INFO - Calculating SHAP values...
2025-11-10 11:41:57,031 - ERROR - Hiba a SHAP magyarázat generálása során (random_forest): Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.254216, while the model output was 0.234216. If this difference is acceptable you can set check_additivity=False to disable this check.
Traceback (most recent call last):
  File "e:\Users\G9179\source\xai_fraud_detection_benchmark\src\explain.py", line 67, in generate_shap_values
    shap_values_obj = explainer(X_to_explain)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 380, in __call__
    v = self.shap_values(X, y=y, from_call=True, check_additivity=check_additivity, approximate=approximate)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 677, in shap_values
    self.assert_additivity(out, self.model.predict(X))
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 862, in assert_additivity
    check_sum(self.expected_value[i] + phi[i].sum(-1), model_output[:, i])
    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 858, in check_sum
    raise ExplainerError(err_msg)
shap.utils._exceptions.ExplainerError: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.254216, while the model output was 0.234216. If this difference is acceptable you can set check_additivity=False to disable this check.
2025-11-10 11:41:57,041 - INFO - SHAP magyarázat elmentve.
2025-11-10 11:41:57,041 - INFO - --- Model: gradient_boosting ---
2025-11-10 11:41:57,042 - INFO - Initialize model parameters from config...
2025-11-10 11:41:57,042 - INFO - Training and evaluating...
2025-11-10 11:41:57,042 - INFO - Starting model training: GradientBoostingClassifier...
2025-11-10 11:42:32,412 - INFO - GradientBoostingClassifier training finished.
2025-11-10 11:42:32,412 - INFO - Prediction on test dataset
2025-11-10 11:42:32,437 - INFO - Metrikák számítása...
2025-11-10 11:42:32,444 - INFO - Results (gradient_boosting): {'accuracy': 0.9695, 'f1_score': 0.9637639252512067, 'pr_auc': 0.36632971755151017}
2025-11-10 11:42:32,450 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\gradient_boosting.pkl
2025-11-10 11:42:32,450 - INFO - SHAP magyarázat generálása (gradient_boosting)...
2025-11-10 11:42:32,450 - INFO - Starting SHAP explanation on model: gradient_boosting...
2025-11-10 11:42:32,460 - INFO - Calculating SHAP values...
2025-11-10 11:42:35,362 - ERROR - Hiba a SHAP magyarázat generálása során (gradient_boosting): Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 4.838153, while the model output was 4.939068. If this difference is acceptable you can set check_additivity=False to disable this check.
Traceback (most recent call last):
  File "e:\Users\G9179\source\xai_fraud_detection_benchmark\src\explain.py", line 67, in generate_shap_values
    shap_values_obj = explainer(X_to_explain)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 380, in __call__
    v = self.shap_values(X, y=y, from_call=True, check_additivity=check_additivity, approximate=approximate)
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 677, in shap_values
    self.assert_additivity(out, self.model.predict(X))
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 864, in assert_additivity
    check_sum(self.expected_value + phi.sum(-1), model_output)
    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\site-packages\shap\explainers\_tree.py", line 858, in check_sum
    raise ExplainerError(err_msg)
shap.utils._exceptions.ExplainerError: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 4.838153, while the model output was 4.939068. If this difference is acceptable you can set check_additivity=False to disable this check.
2025-11-10 11:42:35,367 - INFO - SHAP magyarázat elmentve.
2025-11-10 11:42:35,368 - INFO - Benchmarking finished. Saving results...
2025-11-10 11:42:35,369 - INFO - Metrics summary saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\benchmark_summary.json
2025-11-10 11:42:35,369 - INFO - Benchmarking finished.
2025-11-10 11:45:39,237 - INFO - Strating benchmarking...
2025-11-10 11:45:39,237 - INFO - Random seed beállítva: 42
2025-11-10 11:45:39,238 - INFO - Load and preprocess data...
2025-11-10 11:45:39,238 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-10 11:45:39,689 - INFO - Nyers adat sikeresen betöltve innen: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-10 11:45:39,764 - INFO - Adatok szétválasztva. Train méret: (8000, 393), Test méret: (2000, 393)
2025-11-10 11:45:39,787 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-10 11:45:39,789 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-10 11:45:39,790 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-10 11:45:40,501 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-10 11:45:40,568 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-10 11:45:48,800 - INFO - Feldolgozott train/test szettek elmentve a(z) 'E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-10 11:45:48,815 - INFO - Successfuly loaded data. Train size: (8000, 505), Test size: (2000, 505)
2025-11-10 11:45:48,815 - INFO - Starting iteration with following models: ['logistic_regression', 'random_forest', 'gradient_boosting']
2025-11-10 11:45:48,815 - INFO - --- Model: logistic_regression ---
2025-11-10 11:45:48,816 - INFO - Initialize model parameters from config...
2025-11-10 11:45:48,816 - INFO - Training and evaluating...
2025-11-10 11:45:48,816 - INFO - Starting model training: LogisticRegression...
2025-11-10 11:45:56,200 - INFO - LogisticRegression training finished.
2025-11-10 11:45:56,201 - INFO - Prediction on test dataset
2025-11-10 11:45:56,212 - INFO - Metrikák számítása...
2025-11-10 11:45:56,220 - INFO - Results (logistic_regression): {'accuracy': 0.9645, 'f1_score': 0.9599949731165981, 'pr_auc': 0.27440597908410147}
2025-11-10 11:45:56,223 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\logistic_regression.pkl
2025-11-10 11:45:56,223 - INFO - SHAP magyarázat generálása (logistic_regression)...
2025-11-10 11:45:56,224 - INFO - Starting SHAP explanation on model: logistic_regression...
2025-11-10 11:45:56,226 - INFO - Calculating SHAP values...
2025-11-10 11:45:56,314 - INFO - SHAP calculation finished.
2025-11-10 11:45:56,314 - INFO - Generating SHAP beeswarm plot...
2025-11-10 11:45:56,610 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_beeswarm.png
2025-11-10 11:45:56,611 - INFO - Generating SHAP bar plot...
2025-11-10 11:45:56,900 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_bar.png
2025-11-10 11:45:56,904 - INFO - SHAP magyarázat elmentve.
2025-11-10 11:45:56,904 - INFO - --- Model: random_forest ---
2025-11-10 11:45:56,904 - INFO - Initialize model parameters from config...
2025-11-10 11:45:56,905 - INFO - Training and evaluating...
2025-11-10 11:45:56,905 - INFO - Starting model training: RandomForestClassifier...
2025-11-10 11:45:59,864 - INFO - RandomForestClassifier training finished.
2025-11-10 11:45:59,865 - INFO - Prediction on test dataset
2025-11-10 11:45:59,968 - INFO - Metrikák számítása...
2025-11-10 11:45:59,975 - INFO - Results (random_forest): {'accuracy': 0.9705, 'f1_score': 0.9617141505914562, 'pr_auc': 0.3807479382619139}
2025-11-10 11:45:59,982 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\random_forest.pkl
2025-11-10 11:45:59,983 - INFO - SHAP magyarázat generálása (random_forest)...
2025-11-10 11:45:59,983 - INFO - Starting SHAP explanation on model: random_forest...
2025-11-10 11:45:59,995 - INFO - Calculating SHAP values...
2025-11-10 11:46:15,253 - INFO - SHAP calculation finished.
2025-11-10 11:46:15,254 - INFO - Generating SHAP beeswarm plot...
2025-11-10 11:46:15,807 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\random_forest_shap_beeswarm.png
2025-11-10 11:46:15,807 - INFO - Generating SHAP bar plot...
2025-11-10 11:46:16,393 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\random_forest_shap_bar.png
2025-11-10 11:46:16,396 - INFO - SHAP magyarázat elmentve.
2025-11-10 11:46:16,396 - INFO - --- Model: gradient_boosting ---
2025-11-10 11:46:16,397 - INFO - Initialize model parameters from config...
2025-11-10 11:46:16,397 - INFO - Training and evaluating...
2025-11-10 11:46:16,397 - INFO - Starting model training: GradientBoostingClassifier...
2025-11-10 11:46:54,192 - INFO - GradientBoostingClassifier training finished.
2025-11-10 11:46:54,193 - INFO - Prediction on test dataset
2025-11-10 11:46:54,232 - INFO - Metrikák számítása...
2025-11-10 11:46:54,242 - INFO - Results (gradient_boosting): {'accuracy': 0.9695, 'f1_score': 0.9641666355249618, 'pr_auc': 0.3572696762032967}
2025-11-10 11:46:54,251 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\gradient_boosting.pkl
2025-11-10 11:46:54,251 - INFO - SHAP magyarázat generálása (gradient_boosting)...
2025-11-10 11:46:54,251 - INFO - Starting SHAP explanation on model: gradient_boosting...
2025-11-10 11:46:54,260 - INFO - Calculating SHAP values...
2025-11-10 11:46:57,600 - INFO - SHAP calculation finished.
2025-11-10 11:46:57,600 - INFO - Generating SHAP beeswarm plot...
2025-11-10 11:46:57,862 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\gradient_boosting_shap_beeswarm.png
2025-11-10 11:46:57,863 - INFO - Generating SHAP bar plot...
2025-11-10 11:46:58,133 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\gradient_boosting_shap_bar.png
2025-11-10 11:46:58,135 - INFO - SHAP magyarázat elmentve.
2025-11-10 11:46:58,136 - INFO - Benchmarking finished. Saving results...
2025-11-10 11:46:58,137 - INFO - Metrics summary saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\benchmark_summary.json
2025-11-10 11:46:58,138 - INFO - Benchmarking finished.
2025-11-10 11:57:08,538 - INFO - Strating benchmarking...
2025-11-10 11:57:08,539 - INFO - Random seed beállítva: 42
2025-11-10 11:57:08,540 - INFO - Load and preprocess data...
2025-11-10 11:57:08,541 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-10 11:57:09,016 - INFO - Nyers adat sikeresen betöltve innen: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-10 11:57:09,087 - INFO - Adatok szétválasztva. Train méret: (8000, 393), Test méret: (2000, 393)
2025-11-10 11:57:09,111 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-10 11:57:09,114 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-10 11:57:09,115 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-10 11:57:09,953 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-10 11:57:10,016 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-10 11:57:18,184 - INFO - Feldolgozott train/test szettek elmentve a(z) 'E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-10 11:57:18,197 - INFO - Successfuly loaded data. Train size: (8000, 505), Test size: (2000, 505)
2025-11-10 11:57:18,198 - INFO - Starting iteration with following models: ['logistic_regression', 'random_forest', 'gradient_boosting']
2025-11-10 11:57:18,199 - INFO - --- Model: logistic_regression ---
2025-11-10 11:57:18,199 - INFO - Initialize model parameters from config...
2025-11-10 11:57:18,199 - INFO - Training and evaluating...
2025-11-10 11:57:18,200 - INFO - Starting model training: LogisticRegression...
2025-11-10 11:57:25,394 - INFO - LogisticRegression training finished.
2025-11-10 11:57:25,395 - INFO - Prediction on test dataset
2025-11-10 11:57:25,407 - INFO - Metrikák számítása...
2025-11-10 11:57:25,417 - INFO - Results (logistic_regression): {'accuracy': 0.9645, 'f1_score': 0.9599949731165981, 'pr_auc': 0.27440597908410147}
2025-11-10 11:57:25,419 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\logistic_regression.pkl
2025-11-10 11:57:25,419 - INFO - SHAP magyarázat generálása (logistic_regression)...
2025-11-10 11:57:25,420 - INFO - Starting SHAP explanation on model: logistic_regression...
2025-11-10 11:57:25,423 - INFO - Calculating SHAP values...
2025-11-10 11:57:25,517 - INFO - SHAP calculation finished.
2025-11-10 11:57:25,517 - INFO - Generating SHAP beeswarm plot...
2025-11-10 11:57:25,821 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_beeswarm.png
2025-11-10 11:57:25,822 - INFO - Generating SHAP bar plot...
2025-11-10 11:57:26,122 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_bar.png
2025-11-10 11:57:26,125 - INFO - SHAP magyarázat elmentve.
2025-11-10 11:57:26,126 - INFO - --- Model: random_forest ---
2025-11-10 11:57:26,126 - INFO - Initialize model parameters from config...
2025-11-10 11:57:26,127 - INFO - Training and evaluating...
2025-11-10 11:57:26,128 - INFO - Starting model training: RandomForestClassifier...
2025-11-10 11:57:28,692 - INFO - RandomForestClassifier training finished.
2025-11-10 11:57:28,693 - INFO - Prediction on test dataset
2025-11-10 11:57:28,765 - INFO - Metrikák számítása...
2025-11-10 11:57:28,772 - INFO - Results (random_forest): {'accuracy': 0.9705, 'f1_score': 0.9617141505914562, 'pr_auc': 0.3807479382619139}
2025-11-10 11:57:28,779 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\random_forest.pkl
2025-11-10 11:57:28,779 - INFO - SHAP magyarázat generálása (random_forest)...
2025-11-10 11:57:28,779 - INFO - Starting SHAP explanation on model: random_forest...
2025-11-10 11:57:28,791 - INFO - Calculating SHAP values...
2025-11-10 11:57:42,522 - INFO - SHAP calculation finished.
2025-11-10 11:57:42,523 - INFO - Generating SHAP beeswarm plot...
2025-11-10 11:57:43,012 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\random_forest_shap_beeswarm.png
2025-11-10 11:57:43,013 - INFO - Generating SHAP bar plot...
2025-11-10 11:57:43,519 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\random_forest_shap_bar.png
2025-11-10 11:57:43,522 - INFO - SHAP magyarázat elmentve.
2025-11-10 11:57:43,522 - INFO - --- Model: gradient_boosting ---
2025-11-10 11:57:43,522 - INFO - Initialize model parameters from config...
2025-11-10 11:57:43,522 - INFO - Training and evaluating...
2025-11-10 11:57:43,523 - INFO - Starting model training: GradientBoostingClassifier...
2025-11-10 11:58:26,640 - INFO - GradientBoostingClassifier training finished.
2025-11-10 11:58:26,641 - INFO - Prediction on test dataset
2025-11-10 11:58:26,678 - INFO - Metrikák számítása...
2025-11-10 11:58:26,687 - INFO - Results (gradient_boosting): {'accuracy': 0.9695, 'f1_score': 0.9641666355249618, 'pr_auc': 0.3572696762032967}
2025-11-10 11:58:26,693 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\gradient_boosting.pkl
2025-11-10 11:58:26,693 - INFO - SHAP magyarázat generálása (gradient_boosting)...
2025-11-10 11:58:26,693 - INFO - Starting SHAP explanation on model: gradient_boosting...
2025-11-10 11:58:26,703 - INFO - Calculating SHAP values...
2025-11-10 12:38:19,437 - INFO - Strating benchmarking...
2025-11-10 12:38:19,438 - INFO - Random seed beállítva: 42
2025-11-10 12:38:19,439 - INFO - Load and preprocess data...
2025-11-10 12:38:19,439 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-10 12:38:19,889 - INFO - Nyers adat sikeresen betöltve innen: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-10 12:38:19,960 - INFO - Adatok szétválasztva. Train méret: (8000, 393), Test méret: (2000, 393)
2025-11-10 12:38:19,982 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-10 12:38:19,985 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-10 12:38:19,986 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-10 12:38:20,717 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-10 12:38:20,785 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-10 12:38:28,571 - INFO - Feldolgozott train/test szettek elmentve a(z) 'E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-10 12:38:28,588 - INFO - Successfuly loaded data. Train size: (8000, 505), Test size: (2000, 505)
2025-11-10 12:38:28,588 - INFO - Starting iteration with following models: ['logistic_regression', 'random_forest', 'gradient_boosting']
2025-11-10 12:38:28,588 - INFO - --- Model: logistic_regression ---
2025-11-10 12:38:28,589 - INFO - Initialize model parameters from config...
2025-11-10 12:38:28,589 - INFO - Training and evaluating...
2025-11-10 12:38:28,589 - INFO - Starting model training: LogisticRegression...
2025-11-10 12:38:35,709 - INFO - LogisticRegression training finished.
2025-11-10 12:38:35,709 - INFO - Prediction on test dataset
2025-11-10 12:38:35,720 - INFO - Metrikák számítása...
2025-11-10 12:38:35,726 - INFO - Results (logistic_regression): {'accuracy': 0.9645, 'f1_score': 0.9599949731165981, 'pr_auc': 0.27440597908410147}
2025-11-10 12:38:35,729 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\logistic_regression.pkl
2025-11-10 12:38:35,729 - INFO - SHAP magyarázat generálása (logistic_regression)...
2025-11-10 12:38:35,729 - INFO - Starting SHAP explanation on model: logistic_regression...
2025-11-10 12:38:35,732 - INFO - Calculating SHAP values...
2025-11-10 12:38:35,825 - INFO - SHAP calculation finished.
2025-11-10 12:38:35,826 - INFO - Generating SHAP beeswarm plot...
2025-11-10 12:38:36,202 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_beeswarm.png
2025-11-10 12:38:36,202 - INFO - Generating SHAP bar plot...
2025-11-10 12:38:36,488 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_bar.png
2025-11-10 12:38:36,491 - INFO - SHAP magyarázat elmentve.
2025-11-10 12:38:36,492 - INFO - --- Model: random_forest ---
2025-11-10 12:38:36,492 - INFO - Initialize model parameters from config...
2025-11-10 12:38:36,492 - INFO - Training and evaluating...
2025-11-10 12:38:36,493 - INFO - Starting model training: RandomForestClassifier...
2025-11-10 12:38:39,194 - INFO - RandomForestClassifier training finished.
2025-11-10 12:38:39,195 - INFO - Prediction on test dataset
2025-11-10 12:38:39,297 - INFO - Metrikák számítása...
2025-11-10 12:38:39,303 - INFO - Results (random_forest): {'accuracy': 0.9705, 'f1_score': 0.9617141505914562, 'pr_auc': 0.3807479382619139}
2025-11-10 12:38:39,310 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\random_forest.pkl
2025-11-10 12:38:39,311 - INFO - SHAP magyarázat generálása (random_forest)...
2025-11-10 12:38:39,311 - INFO - Starting SHAP explanation on model: random_forest...
2025-11-10 12:38:39,321 - INFO - Calculating SHAP values...
2025-11-10 12:38:52,377 - INFO - SHAP calculation finished.
2025-11-10 12:38:52,377 - INFO - Generating SHAP beeswarm plot...
2025-11-10 12:38:52,990 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\random_forest_shap_beeswarm.png
2025-11-10 12:38:52,990 - INFO - Generating SHAP bar plot...
2025-11-10 12:38:53,558 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\random_forest_shap_bar.png
2025-11-10 12:38:53,561 - INFO - SHAP magyarázat elmentve.
2025-11-10 12:38:53,562 - INFO - --- Model: gradient_boosting ---
2025-11-10 12:38:53,562 - INFO - Initialize model parameters from config...
2025-11-10 12:38:53,562 - INFO - Training and evaluating...
2025-11-10 12:38:53,562 - INFO - Starting model training: GradientBoostingClassifier...
2025-11-10 12:39:31,589 - INFO - GradientBoostingClassifier training finished.
2025-11-10 12:39:31,590 - INFO - Prediction on test dataset
2025-11-10 12:39:31,625 - INFO - Metrikák számítása...
2025-11-10 12:39:31,632 - INFO - Results (gradient_boosting): {'accuracy': 0.9695, 'f1_score': 0.9641666355249618, 'pr_auc': 0.3572696762032967}
2025-11-10 12:39:31,638 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\gradient_boosting.pkl
2025-11-10 12:39:31,639 - INFO - SHAP magyarázat generálása (gradient_boosting)...
2025-11-10 12:39:31,639 - INFO - Starting SHAP explanation on model: gradient_boosting...
2025-11-10 12:39:31,649 - INFO - Calculating SHAP values...
2025-11-10 12:39:34,929 - INFO - SHAP calculation finished.
2025-11-10 12:39:34,930 - INFO - Generating SHAP beeswarm plot...
2025-11-10 12:39:35,204 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\gradient_boosting_shap_beeswarm.png
2025-11-10 12:39:35,205 - INFO - Generating SHAP bar plot...
2025-11-10 12:39:35,519 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\gradient_boosting_shap_bar.png
2025-11-10 12:39:35,522 - INFO - SHAP magyarázat elmentve.
2025-11-10 12:39:35,522 - INFO - Benchmarking finished. Saving results...
2025-11-10 12:39:35,524 - INFO - Metrics summary saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\benchmark_summary.json
2025-11-10 12:39:35,524 - INFO - Benchmarking finished.
2025-11-10 12:45:29,273 - INFO - Strating benchmarking...
2025-11-10 12:45:29,273 - INFO - Random seed beállítva: 42
2025-11-10 12:45:29,274 - INFO - Load and preprocess data...
2025-11-10 12:45:29,275 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-10 12:45:29,700 - INFO - Nyers adat sikeresen betöltve innen: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-10 12:45:29,773 - INFO - Adatok szétválasztva. Train méret: (8000, 393), Test méret: (2000, 393)
2025-11-10 12:45:29,794 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-10 12:45:29,796 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-10 12:45:29,797 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-10 12:45:30,433 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-10 12:45:30,493 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-10 12:45:37,863 - INFO - Feldolgozott train/test szettek elmentve a(z) 'E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-10 12:45:37,881 - INFO - Successfuly loaded data. Train size: (8000, 505), Test size: (2000, 505)
2025-11-10 12:45:37,881 - INFO - Starting iteration with following models: ['logistic_regression', 'random_forest', 'lof']
2025-11-10 12:45:37,882 - INFO - --- Model: logistic_regression ---
2025-11-10 12:45:37,882 - INFO - Initialize model parameters from config...
2025-11-10 12:45:37,882 - INFO - Training and evaluating...
2025-11-10 12:45:37,882 - INFO - Starting model training: LogisticRegression...
2025-11-10 12:45:44,598 - INFO - LogisticRegression training finished.
2025-11-10 12:45:44,599 - INFO - Prediction on test dataset
2025-11-10 12:45:44,610 - INFO - Metrikák számítása...
2025-11-10 12:45:44,617 - INFO - Results (logistic_regression): {'accuracy': 0.9645, 'f1_score': 0.9599949731165981, 'pr_auc': 0.27440597908410147}
2025-11-10 12:45:44,618 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\logistic_regression.pkl
2025-11-10 12:45:44,619 - INFO - SHAP magyarázat generálása (logistic_regression)...
2025-11-10 12:45:44,619 - INFO - Starting SHAP explanation on model: logistic_regression...
2025-11-10 12:45:44,621 - INFO - Calculating SHAP values...
2025-11-10 12:45:44,703 - INFO - SHAP calculation finished.
2025-11-10 12:45:44,703 - INFO - Generating SHAP beeswarm plot...
2025-11-10 12:45:44,984 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_beeswarm.png
2025-11-10 12:45:44,984 - INFO - Generating SHAP bar plot...
2025-11-10 12:45:45,227 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_bar.png
2025-11-10 12:45:45,230 - INFO - SHAP magyarázat elmentve.
2025-11-10 12:45:45,231 - INFO - --- Model: random_forest ---
2025-11-10 12:45:45,231 - INFO - Initialize model parameters from config...
2025-11-10 12:45:45,231 - INFO - Training and evaluating...
2025-11-10 12:45:45,232 - INFO - Starting model training: RandomForestClassifier...
2025-11-10 12:45:47,588 - INFO - RandomForestClassifier training finished.
2025-11-10 12:45:47,589 - INFO - Prediction on test dataset
2025-11-10 12:45:47,653 - INFO - Metrikák számítása...
2025-11-10 12:45:47,659 - INFO - Results (random_forest): {'accuracy': 0.9705, 'f1_score': 0.9617141505914562, 'pr_auc': 0.3807479382619139}
2025-11-10 12:45:47,666 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\random_forest.pkl
2025-11-10 12:45:47,666 - INFO - SHAP magyarázat generálása (random_forest)...
2025-11-10 12:45:47,667 - INFO - Starting SHAP explanation on model: random_forest...
2025-11-10 12:45:47,677 - INFO - Calculating SHAP values...
2025-11-10 12:46:00,365 - INFO - SHAP calculation finished.
2025-11-10 12:46:00,365 - INFO - Generating SHAP beeswarm plot...
2025-11-10 12:46:00,869 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\random_forest_shap_beeswarm.png
2025-11-10 12:46:00,870 - INFO - Generating SHAP bar plot...
2025-11-10 12:46:01,371 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\random_forest_shap_bar.png
2025-11-10 12:46:01,374 - INFO - SHAP magyarázat elmentve.
2025-11-10 12:46:01,374 - INFO - --- Model: lof ---
2025-11-10 12:46:01,374 - INFO - Initialize model parameters from config...
2025-11-10 12:46:01,374 - INFO - Training and evaluating...
2025-11-10 12:46:01,374 - INFO - Starting model training: LocalOutlierFactor...
2025-11-10 12:46:04,631 - INFO - LocalOutlierFactor training finished.
2025-11-10 12:46:04,632 - INFO - Prediction on test dataset
2025-11-10 12:46:04,632 - ERROR - Error with model: lof: This 'LocalOutlierFactor' has no attribute 'predict'
2025-11-10 12:46:04,633 - INFO - Benchmarking finished. Saving results...
2025-11-10 12:46:04,634 - INFO - Metrics summary saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\benchmark_summary.json
2025-11-10 12:46:04,634 - INFO - Benchmarking finished.
2025-11-10 15:16:32,266 - INFO - Strating benchmarking...
2025-11-10 15:16:32,266 - INFO - Random seed beállítva: 42
2025-11-10 15:16:32,268 - INFO - Load and preprocess data...
2025-11-10 15:16:32,268 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-10 15:16:32,697 - INFO - Nyers adat sikeresen betöltve innen: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-10 15:16:32,758 - INFO - Adatok szétválasztva. Train méret: (8000, 393), Test méret: (2000, 393)
2025-11-10 15:16:32,782 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-10 15:16:32,785 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-10 15:16:32,786 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-10 15:16:33,424 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-10 15:16:33,488 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-10 15:16:40,997 - INFO - Feldolgozott train/test szettek elmentve a(z) 'E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-10 15:16:41,010 - INFO - Successfuly loaded data. Train size: (8000, 505), Test size: (2000, 505)
2025-11-10 15:17:27,066 - INFO - Strating benchmarking...
2025-11-10 15:17:27,067 - INFO - Random seed beállítva: 42
2025-11-10 15:17:27,068 - INFO - Load and preprocess data...
2025-11-10 15:17:27,068 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-10 15:17:27,516 - INFO - Nyers adat sikeresen betöltve innen: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-10 15:17:27,577 - INFO - Adatok szétválasztva. Train méret: (8000, 393), Test méret: (2000, 393)
2025-11-10 15:17:27,599 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-10 15:17:27,602 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-10 15:17:27,603 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-10 15:17:28,217 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-10 15:17:28,275 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-10 15:17:35,802 - INFO - Feldolgozott train/test szettek elmentve a(z) 'E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-10 15:17:35,815 - INFO - Successfuly loaded data. Train size: (8000, 505), Test size: (2000, 505)
2025-11-10 15:17:35,815 - INFO - Starting iteration with following models: (['logistic_regression', 'random_forest'], ['lof'])
2025-11-10 15:19:27,182 - INFO - Strating benchmarking...
2025-11-10 15:19:27,182 - INFO - Random seed beállítva: 42
2025-11-10 15:19:27,183 - INFO - Load and preprocess data...
2025-11-10 15:19:27,184 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-10 15:19:27,622 - INFO - Nyers adat sikeresen betöltve innen: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-10 15:19:27,693 - INFO - Adatok szétválasztva. Train méret: (8000, 393), Test méret: (2000, 393)
2025-11-10 15:19:27,718 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-10 15:19:27,721 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-10 15:19:27,722 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-10 15:19:28,398 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-10 15:19:28,466 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-10 15:19:36,172 - INFO - Feldolgozott train/test szettek elmentve a(z) 'E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-10 15:19:36,186 - INFO - Successfuly loaded data. Train size: (8000, 505), Test size: (2000, 505)
2025-11-10 15:19:36,187 - INFO - Starting iteration with following models: ['logistic_regression', 'random_forest', 'lof']
2025-11-10 15:19:36,188 - INFO - --- Model: logistic_regression ---
2025-11-10 15:19:36,188 - INFO - Initialize model parameters from config...
2025-11-10 15:19:36,188 - INFO - Training and evaluating (supervised)...
2025-11-10 15:19:36,189 - INFO - Starting model training: LogisticRegression...
2025-11-10 15:19:43,618 - INFO - LogisticRegression training finished.
2025-11-10 15:19:43,619 - INFO - Prediction on test dataset
2025-11-10 15:19:43,636 - INFO - Metrikák számítása...
2025-11-10 15:19:43,646 - INFO - Results (logistic_regression): {'accuracy': 0.9645, 'f1_score': 0.9599949731165981, 'pr_auc': 0.27440597908410147}
2025-11-10 15:19:43,648 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\logistic_regression.pkl
2025-11-10 15:19:43,649 - INFO - SHAP magyarázat generálása (logistic_regression)...
2025-11-10 15:19:43,649 - INFO - Starting SHAP explanation on model: logistic_regression...
2025-11-10 15:19:43,651 - INFO - Calculating SHAP values...
2025-11-10 15:19:43,755 - INFO - SHAP calculation finished.
2025-11-10 15:19:43,756 - INFO - Generating SHAP beeswarm plot...
2025-11-10 15:19:44,116 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_beeswarm.png
2025-11-10 15:19:44,117 - INFO - Generating SHAP bar plot...
2025-11-10 15:19:44,369 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_bar.png
2025-11-10 15:19:44,372 - INFO - SHAP magyarázat elmentve.
2025-11-10 15:19:44,373 - INFO - --- Model: random_forest ---
2025-11-10 15:19:44,373 - INFO - Initialize model parameters from config...
2025-11-10 15:19:44,374 - INFO - Training and evaluating (supervised)...
2025-11-10 15:19:44,382 - INFO - Starting model training: RandomForestClassifier...
2025-11-10 15:19:47,158 - INFO - RandomForestClassifier training finished.
2025-11-10 15:19:47,158 - INFO - Prediction on test dataset
2025-11-10 15:19:47,232 - INFO - Metrikák számítása...
2025-11-10 15:19:47,238 - INFO - Results (random_forest): {'accuracy': 0.9705, 'f1_score': 0.9617141505914562, 'pr_auc': 0.3807479382619139}
2025-11-10 15:19:47,246 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\random_forest.pkl
2025-11-10 15:19:47,246 - INFO - SHAP magyarázat generálása (random_forest)...
2025-11-10 15:19:47,246 - INFO - Starting SHAP explanation on model: random_forest...
2025-11-10 15:19:47,258 - INFO - Calculating SHAP values...
2025-11-10 15:20:00,251 - INFO - SHAP calculation finished.
2025-11-10 15:20:00,252 - INFO - Generating SHAP beeswarm plot...
2025-11-10 15:20:01,061 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\random_forest_shap_beeswarm.png
2025-11-10 15:20:01,062 - INFO - Generating SHAP bar plot...
2025-11-10 15:20:01,913 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\random_forest_shap_bar.png
2025-11-10 15:20:01,919 - INFO - SHAP magyarázat elmentve.
2025-11-10 15:20:01,920 - INFO - --- Model: lof ---
2025-11-10 15:20:01,920 - INFO - Initialize model parameters from config...
2025-11-10 15:20:01,921 - INFO - Training and evaluating (supervised)...
2025-11-10 15:20:01,943 - INFO - Starting UNSUPERVISED model training: LocalOutlierFactor...
2025-11-10 15:20:05,638 - INFO - LocalOutlierFactor training finished.
2025-11-10 15:20:05,638 - INFO - Prediction on test dataset (unsupervised)
2025-11-10 15:20:05,639 - ERROR - Error with model: lof: This 'LocalOutlierFactor' has no attribute 'predict'
2025-11-10 15:20:05,639 - INFO - Benchmarking finished. Saving results...
2025-11-10 15:20:05,640 - INFO - Metrics summary saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\benchmark_summary.json
2025-11-10 15:20:05,641 - INFO - Benchmarking finished.
2025-11-10 15:27:22,510 - INFO - Strating benchmarking...
2025-11-10 15:27:22,511 - INFO - Random seed beállítva: 42
2025-11-10 15:27:22,512 - INFO - Load and preprocess data...
2025-11-10 15:27:22,512 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-10 15:27:23,011 - INFO - Nyers adat sikeresen betöltve innen: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-10 15:27:23,088 - INFO - Adatok szétválasztva. Train méret: (8000, 393), Test méret: (2000, 393)
2025-11-10 15:27:23,111 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-10 15:27:23,113 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-10 15:27:23,114 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-10 15:27:23,842 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-10 15:27:23,908 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-10 15:27:31,568 - INFO - Feldolgozott train/test szettek elmentve a(z) 'E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-10 15:27:31,587 - INFO - Successfuly loaded data. Train size: (8000, 505), Test size: (2000, 505)
2025-11-10 15:27:31,588 - INFO - Starting iteration with following models: ['logistic_regression', 'random_forest', 'lof']
2025-11-10 15:27:31,588 - INFO - --- Model: logistic_regression ---
2025-11-10 15:27:31,589 - INFO - Initialize model parameters from config...
2025-11-10 15:27:31,589 - INFO - Training and evaluating (supervised)...
2025-11-10 15:27:31,589 - INFO - Starting model training: LogisticRegression...
2025-11-10 15:27:38,705 - INFO - LogisticRegression training finished.
2025-11-10 15:27:38,705 - INFO - Prediction on test dataset
2025-11-10 15:27:38,718 - INFO - Metrikák számítása...
2025-11-10 15:27:38,725 - INFO - Results (logistic_regression): {'accuracy': 0.9645, 'f1_score': 0.9599949731165981, 'pr_auc': 0.27440597908410147}
2025-11-10 15:27:38,727 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\logistic_regression.pkl
2025-11-10 15:27:38,727 - INFO - SHAP magyarázat generálása (logistic_regression)...
2025-11-10 15:27:38,727 - INFO - Starting SHAP explanation on model: logistic_regression...
2025-11-10 15:27:38,729 - INFO - Calculating SHAP values...
2025-11-10 15:27:38,809 - INFO - SHAP calculation finished.
2025-11-10 15:27:38,810 - INFO - Generating SHAP beeswarm plot...
2025-11-10 15:27:39,123 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_beeswarm.png
2025-11-10 15:27:39,124 - INFO - Generating SHAP bar plot...
2025-11-10 15:27:39,396 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_bar.png
2025-11-10 15:27:39,398 - INFO - SHAP magyarázat elmentve.
2025-11-10 15:27:39,399 - INFO - --- Model: random_forest ---
2025-11-10 15:27:39,399 - INFO - Initialize model parameters from config...
2025-11-10 15:27:39,399 - INFO - Training and evaluating (supervised)...
2025-11-10 15:27:39,400 - INFO - Starting model training: RandomForestClassifier...
2025-11-10 15:27:41,900 - INFO - RandomForestClassifier training finished.
2025-11-10 15:27:41,901 - INFO - Prediction on test dataset
2025-11-10 15:27:41,991 - INFO - Metrikák számítása...
2025-11-10 15:27:41,998 - INFO - Results (random_forest): {'accuracy': 0.9705, 'f1_score': 0.9617141505914562, 'pr_auc': 0.3807479382619139}
2025-11-10 15:27:42,006 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\random_forest.pkl
2025-11-10 15:27:42,006 - INFO - SHAP magyarázat generálása (random_forest)...
2025-11-10 15:27:42,007 - INFO - Starting SHAP explanation on model: random_forest...
2025-11-10 15:27:42,019 - INFO - Calculating SHAP values...
2025-11-10 15:27:54,842 - INFO - SHAP calculation finished.
2025-11-10 15:27:54,842 - INFO - Generating SHAP beeswarm plot...
2025-11-10 15:27:55,377 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\random_forest_shap_beeswarm.png
2025-11-10 15:27:55,377 - INFO - Generating SHAP bar plot...
2025-11-10 15:27:55,867 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\random_forest_shap_bar.png
2025-11-10 15:27:55,870 - INFO - SHAP magyarázat elmentve.
2025-11-10 15:27:55,871 - INFO - --- Model: lof ---
2025-11-10 15:27:55,871 - INFO - Initialize model parameters from config...
2025-11-10 15:27:55,871 - INFO - Training and evaluating (supervised)...
2025-11-10 15:27:55,872 - INFO - Starting UNSUPERVISED model training: LocalOutlierFactor...
2025-11-10 15:27:58,971 - INFO - LocalOutlierFactor training finished.
2025-11-10 15:27:58,972 - INFO - Prediction on test dataset (unsupervised)
2025-11-10 15:28:00,046 - INFO - Metrikák számítása (unsupervised mapped)...
2025-11-10 15:28:00,052 - INFO - Results (lof): {'accuracy': 0.7725, 'f1_score': 0.8434778785341934, 'pr_auc': 0.10849819937636335}
2025-11-10 15:28:00,164 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\lof.pkl
2025-11-10 15:28:00,164 - INFO - SHAP magyarázat generálása (lof)...
2025-11-10 15:28:00,164 - INFO - Starting SHAP explanation on model: lof...
2025-11-10 15:28:00,166 - WARNING - SHAP: No specific explainer for LocalOutlierFactor. Using slow KernelExplainer.
2025-11-10 15:28:00,166 - ERROR - Hiba a SHAP magyarázat generálása során (lof): 'LocalOutlierFactor' object has no attribute 'predict_proba'
Traceback (most recent call last):
  File "e:\Users\G9179\source\xai_fraud_detection_benchmark\src\explain.py", line 63, in generate_shap_values
    explainer = shap.KernelExplainer(model.predict_proba, X_background)
                                     ^^^^^^^^^^^^^^^^^^^
AttributeError: 'LocalOutlierFactor' object has no attribute 'predict_proba'
2025-11-10 15:28:00,168 - INFO - SHAP magyarázat elmentve.
2025-11-10 15:28:00,168 - INFO - Benchmarking finished. Saving results...
2025-11-10 15:28:00,169 - INFO - Metrics summary saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\benchmark_summary.json
2025-11-10 15:28:00,170 - INFO - Benchmarking finished.
2025-11-10 15:38:48,720 - INFO - Strating benchmarking...
2025-11-10 15:38:48,721 - INFO - Random seed beállítva: 42
2025-11-10 15:38:48,721 - INFO - Load and preprocess data...
2025-11-10 15:38:48,722 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-10 15:38:49,169 - INFO - Nyers adat sikeresen betöltve innen: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-10 15:38:49,228 - INFO - Adatok szétválasztva. Train méret: (8000, 393), Test méret: (2000, 393)
2025-11-10 15:38:49,249 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-10 15:38:49,251 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-10 15:38:49,252 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-10 15:38:49,861 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-10 15:38:49,929 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-10 15:38:57,267 - INFO - Feldolgozott train/test szettek elmentve a(z) 'E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-10 15:38:57,280 - INFO - Successfuly loaded data. Train size: (8000, 505), Test size: (2000, 505)
2025-11-10 15:38:57,281 - INFO - Starting iteration with following models: ['logistic_regression', 'random_forest', 'lof']
2025-11-10 15:38:57,281 - INFO - --- Model: logistic_regression ---
2025-11-10 15:38:57,281 - INFO - Initialize model parameters from config...
2025-11-10 15:38:57,282 - INFO - Training and evaluating (supervised)...
2025-11-10 15:38:57,282 - INFO - Starting model training: LogisticRegression...
2025-11-10 15:39:04,120 - INFO - LogisticRegression training finished.
2025-11-10 15:39:04,121 - INFO - Prediction on test dataset
2025-11-10 15:39:04,132 - INFO - Metrikák számítása...
2025-11-10 15:39:04,138 - INFO - Results (logistic_regression): {'accuracy': 0.9645, 'f1_score': 0.9599949731165981, 'pr_auc': 0.27440597908410147}
2025-11-10 15:39:04,140 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\logistic_regression.pkl
2025-11-10 15:39:04,140 - INFO - SHAP magyarázat generálása (logistic_regression)...
2025-11-10 15:39:04,141 - INFO - Starting SHAP explanation on model: logistic_regression...
2025-11-10 15:39:04,142 - INFO - Calculating SHAP values...
2025-11-10 15:39:04,219 - INFO - SHAP calculation finished.
2025-11-10 15:39:04,219 - INFO - Generating SHAP beeswarm plot...
2025-11-10 15:39:04,513 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_beeswarm.png
2025-11-10 15:39:04,515 - INFO - Generating SHAP bar plot...
2025-11-10 15:39:04,752 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_bar.png
2025-11-10 15:39:04,756 - INFO - SHAP magyarázat elmentve.
2025-11-10 15:39:04,756 - INFO - LIME magyarázat generálása (logistic_regression)...
2025-11-10 15:39:04,756 - ERROR - Error with model: logistic_regression: generate_lime_explanation() got an unexpected keyword argument 'X_train_full'
2025-11-10 15:39:04,757 - INFO - --- Model: random_forest ---
2025-11-10 15:39:04,757 - INFO - Initialize model parameters from config...
2025-11-10 15:39:04,757 - INFO - Training and evaluating (supervised)...
2025-11-10 15:39:04,757 - INFO - Starting model training: RandomForestClassifier...
2025-11-10 15:39:07,046 - INFO - RandomForestClassifier training finished.
2025-11-10 15:39:07,046 - INFO - Prediction on test dataset
2025-11-10 15:39:07,108 - INFO - Metrikák számítása...
2025-11-10 15:39:07,113 - INFO - Results (random_forest): {'accuracy': 0.9705, 'f1_score': 0.9617141505914562, 'pr_auc': 0.3807479382619139}
2025-11-10 15:39:07,120 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\random_forest.pkl
2025-11-10 15:39:07,120 - INFO - SHAP magyarázat generálása (random_forest)...
2025-11-10 15:39:07,121 - INFO - Starting SHAP explanation on model: random_forest...
2025-11-10 15:39:07,134 - INFO - Calculating SHAP values...
2025-11-10 15:39:20,150 - INFO - SHAP calculation finished.
2025-11-10 15:39:20,150 - INFO - Generating SHAP beeswarm plot...
2025-11-10 15:39:20,650 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\random_forest_shap_beeswarm.png
2025-11-10 15:39:20,651 - INFO - Generating SHAP bar plot...
2025-11-10 15:39:21,183 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\random_forest_shap_bar.png
2025-11-10 15:39:21,186 - INFO - SHAP magyarázat elmentve.
2025-11-10 15:39:21,187 - INFO - LIME magyarázat generálása (random_forest)...
2025-11-10 15:39:21,187 - ERROR - Error with model: random_forest: generate_lime_explanation() got an unexpected keyword argument 'X_train_full'
2025-11-10 15:39:21,187 - INFO - --- Model: lof ---
2025-11-10 15:39:21,188 - INFO - Initialize model parameters from config...
2025-11-10 15:39:21,188 - INFO - Training and evaluating (supervised)...
2025-11-10 15:39:21,188 - INFO - Starting UNSUPERVISED model training: LocalOutlierFactor...
2025-11-10 15:40:07,121 - INFO - Strating benchmarking...
2025-11-10 15:40:07,121 - INFO - Random seed beállítva: 42
2025-11-10 15:40:07,122 - INFO - Load and preprocess data...
2025-11-10 15:40:07,123 - INFO - Adat-elõkészítési folyamat indítása...
2025-11-10 15:40:07,632 - INFO - Nyers adat sikeresen betöltve innen: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\raw\ieee_fraud.csv
2025-11-10 15:40:07,698 - INFO - Adatok szétválasztva. Train méret: (8000, 393), Test méret: (2000, 393)
2025-11-10 15:40:07,719 - INFO - Azonosított numerikus oszlopok: ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']
2025-11-10 15:40:07,723 - INFO - Azonosított kategorikus oszlopok: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']
2025-11-10 15:40:07,724 - INFO - Preprocessing pipeline illesztése (fit) a train adatokra...
2025-11-10 15:40:08,363 - INFO - Preprocessing pipeline alkalmazása (transform) a test adatokra...
2025-11-10 15:40:08,435 - INFO - Feldolgozó pipeline (preprocessor) elmentve ide: E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed\preprocessor.pkl
2025-11-10 15:40:16,028 - INFO - Feldolgozott train/test szettek elmentve a(z) 'E:\Users\G9179\source\xai_fraud_detection_benchmark\data\processed' mappába.
2025-11-10 15:40:16,049 - INFO - Successfuly loaded data. Train size: (8000, 505), Test size: (2000, 505)
2025-11-10 15:40:16,049 - INFO - Starting iteration with following models: ['logistic_regression', 'random_forest', 'lof']
2025-11-10 15:40:16,050 - INFO - --- Model: logistic_regression ---
2025-11-10 15:40:16,050 - INFO - Initialize model parameters from config...
2025-11-10 15:40:16,050 - INFO - Training and evaluating (supervised)...
2025-11-10 15:40:16,050 - INFO - Starting model training: LogisticRegression...
2025-11-10 15:40:23,153 - INFO - LogisticRegression training finished.
2025-11-10 15:40:23,153 - INFO - Prediction on test dataset
2025-11-10 15:40:23,166 - INFO - Metrikák számítása...
2025-11-10 15:40:23,172 - INFO - Results (logistic_regression): {'accuracy': 0.9645, 'f1_score': 0.9599949731165981, 'pr_auc': 0.27440597908410147}
2025-11-10 15:40:23,174 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\logistic_regression.pkl
2025-11-10 15:40:23,175 - INFO - SHAP magyarázat generálása (logistic_regression)...
2025-11-10 15:40:23,175 - INFO - Starting SHAP explanation on model: logistic_regression...
2025-11-10 15:40:23,178 - INFO - Calculating SHAP values...
2025-11-10 15:40:23,262 - INFO - SHAP calculation finished.
2025-11-10 15:40:23,262 - INFO - Generating SHAP beeswarm plot...
2025-11-10 15:40:23,545 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_beeswarm.png
2025-11-10 15:40:23,546 - INFO - Generating SHAP bar plot...
2025-11-10 15:40:23,800 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\logistic_regression_shap_bar.png
2025-11-10 15:40:23,804 - INFO - SHAP magyarázat elmentve.
2025-11-10 15:40:23,804 - INFO - LIME magyarázat generálása (logistic_regression)...
2025-11-10 15:40:23,804 - INFO - Starting LIME explanation for model: (logistic_regression), index: 123...
2025-11-10 15:40:26,730 - INFO - LIME explanation saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\lime\logistic_regression_lime_instance_123.html
2025-11-10 15:40:26,734 - INFO - LIME magyarázat elmentve.
2025-11-10 15:40:26,734 - INFO - --- Model: random_forest ---
2025-11-10 15:40:26,734 - INFO - Initialize model parameters from config...
2025-11-10 15:40:26,734 - INFO - Training and evaluating (supervised)...
2025-11-10 15:40:26,735 - INFO - Starting model training: RandomForestClassifier...
2025-11-10 15:40:29,296 - INFO - RandomForestClassifier training finished.
2025-11-10 15:40:29,296 - INFO - Prediction on test dataset
2025-11-10 15:40:29,365 - INFO - Metrikák számítása...
2025-11-10 15:40:29,372 - INFO - Results (random_forest): {'accuracy': 0.9715, 'f1_score': 0.9635200917033202, 'pr_auc': 0.39557342902801895}
2025-11-10 15:40:29,378 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\random_forest.pkl
2025-11-10 15:40:29,379 - INFO - SHAP magyarázat generálása (random_forest)...
2025-11-10 15:40:29,379 - INFO - Starting SHAP explanation on model: random_forest...
2025-11-10 15:40:29,391 - INFO - Calculating SHAP values...
2025-11-10 15:40:42,404 - INFO - SHAP calculation finished.
2025-11-10 15:40:42,404 - INFO - Generating SHAP beeswarm plot...
2025-11-10 15:40:42,899 - INFO - SHAP beeswarm plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\random_forest_shap_beeswarm.png
2025-11-10 15:40:42,899 - INFO - Generating SHAP bar plot...
2025-11-10 15:40:43,404 - INFO - SHAP bar plot saved to E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\shap\random_forest_shap_bar.png
2025-11-10 15:40:43,406 - INFO - SHAP magyarázat elmentve.
2025-11-10 15:40:43,407 - INFO - LIME magyarázat generálása (random_forest)...
2025-11-10 15:40:43,407 - INFO - Starting LIME explanation for model: (random_forest), index: 123...
2025-11-10 15:40:46,097 - INFO - LIME explanation saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\explanations\lime\random_forest_lime_instance_123.html
2025-11-10 15:40:46,101 - INFO - LIME magyarázat elmentve.
2025-11-10 15:40:46,101 - INFO - --- Model: lof ---
2025-11-10 15:40:46,101 - INFO - Initialize model parameters from config...
2025-11-10 15:40:46,101 - INFO - Training and evaluating (supervised)...
2025-11-10 15:40:46,102 - INFO - Starting UNSUPERVISED model training: LocalOutlierFactor...
2025-11-10 15:40:49,304 - INFO - LocalOutlierFactor training finished.
2025-11-10 15:40:49,305 - INFO - Prediction on test dataset (unsupervised)
2025-11-10 15:40:50,351 - INFO - Metrikák számítása (unsupervised mapped)...
2025-11-10 15:40:50,358 - INFO - Results (lof): {'accuracy': 0.7725, 'f1_score': 0.8434778785341934, 'pr_auc': 0.10849819937636335}
2025-11-10 15:40:50,488 - INFO - Model saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\models\lof.pkl
2025-11-10 15:40:50,489 - INFO - SHAP magyarázat generálása (lof)...
2025-11-10 15:40:50,489 - INFO - Starting SHAP explanation on model: lof...
2025-11-10 15:40:50,490 - WARNING - SHAP: No specific explainer for LocalOutlierFactor. Using slow KernelExplainer.
2025-11-10 15:40:50,491 - ERROR - Hiba a SHAP magyarázat generálása során (lof): 'LocalOutlierFactor' object has no attribute 'predict_proba'
Traceback (most recent call last):
  File "e:\Users\G9179\source\xai_fraud_detection_benchmark\src\explain.py", line 63, in generate_shap_values
    explainer = shap.KernelExplainer(model.predict_proba, X_background)
                                     ^^^^^^^^^^^^^^^^^^^
AttributeError: 'LocalOutlierFactor' object has no attribute 'predict_proba'
2025-11-10 15:40:50,492 - INFO - SHAP magyarázat elmentve.
2025-11-10 15:40:50,493 - INFO - LIME magyarázat generálása (lof)...
2025-11-10 15:40:50,493 - INFO - Starting LIME explanation for model: (lof), index: 123...
2025-11-10 15:40:51,998 - ERROR - Error during lime explanation: 'LocalOutlierFactor' object has no attribute 'predict_proba'
2025-11-10 15:40:51,999 - INFO - LIME magyarázat elmentve.
2025-11-10 15:40:52,000 - INFO - Benchmarking finished. Saving results...
2025-11-10 15:40:52,001 - INFO - Metrics summary saved to: E:\Users\G9179\source\xai_fraud_detection_benchmark\results\benchmark_summary.json
2025-11-10 15:40:52,015 - INFO - Benchmarking finished.
